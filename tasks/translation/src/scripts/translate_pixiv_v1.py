#!/usr/bin/env python3
"""
æ‰¹é‡ç¿»è¯‘ Pixiv ä¸‹è½½çš„å°è¯´ï¼š
- è¾“å…¥ä¸ºå¸¦ YAML front matter çš„ .txtï¼ˆç”± batch_download_v1.py äº§å‡ºï¼‰
- è¾“å‡ºä¸ºåŒç›®å½• {basename}_zh.txt

å®ç°è¦ç‚¹ï¼š
- è§£æ YAML å¤´ï¼ˆç®€å•è¾¹ç•Œè¯†åˆ«ï¼‰ï¼Œæ­£æ–‡æŒ‰æ®µè½åˆ’åˆ†ï¼ˆè¿ç»­éç©ºä¸ºæ®µï¼‰
- å¤ç”¨ç°æœ‰ test_translation.py çš„ç¿»è¯‘é€»è¾‘ï¼ˆé€šè¿‡æœ¬åœ° OpenAI å…¼å®¹ç«¯ç‚¹ï¼‰
- æ”¯æŒæ–‡ä»¶/ç›®å½•/é€šé…ç¬¦è¾“å…¥ï¼Œå¢é‡è·³è¿‡å·²å­˜åœ¨ _zh.txtï¼ˆé™¤é --overwriteï¼‰
- è®°å½•å®Œæ•´ prompt/response åˆ° logs
"""

from __future__ import annotations

import argparse
import glob
import os
import sys
import time
import re
import yaml
from pathlib import Path
from typing import List, Tuple, Optional, Dict

from openai import OpenAI
from openai import BadRequestError

import logging
from datetime import datetime


def setup_logging(log_path: Optional[Path] = None, stream_output: bool = True) -> logging.Logger:
    """è®¾ç½®ç»Ÿä¸€çš„æ—¥å¿—ç³»ç»Ÿ"""
    logger = logging.getLogger('translation')
    logger.setLevel(logging.DEBUG)
    logger.handlers.clear()  # æ¸…é™¤å·²æœ‰handlers

    formatter = logging.Formatter('%(asctime)s [%(levelname)s] %(message)s', datefmt='%Y-%m-%d %H:%M:%S')

    # æ§åˆ¶å°è¾“å‡º
    if stream_output:
        console_handler = logging.StreamHandler(sys.stdout)
        console_handler.setLevel(logging.INFO)
        console_handler.setFormatter(formatter)
        logger.addHandler(console_handler)

    # æ–‡ä»¶è¾“å‡ºï¼ˆå¦‚æœæœ‰log_pathï¼‰
    if log_path:
        file_handler = logging.FileHandler(log_path, encoding='utf-8')
        file_handler.setLevel(logging.DEBUG)
        file_handler.setFormatter(formatter)
        logger.addHandler(file_handler)

    return logger

def log_message(logger: Optional[logging.Logger], message: str, level: str = "INFO"):
    """ç»Ÿä¸€çš„æ—¥å¿—è¾“å‡ºå‡½æ•°ï¼Œæ”¯æŒloggerä¸ºNoneçš„æƒ…å†µ"""
    if level.upper() == "INFO":
        print(f"[INFO] {message}")
        if logger:
            logger.info(message)
    elif level.upper() == "WARNING":
        print(f"[WARNING] {message}")
        if logger:
            logger.warning(message)
    elif level.upper() == "ERROR":
        print(f"[ERROR] {message}")
        if logger:
            logger.error(message)
    elif level.upper() == "DEBUG":
        print(f"[DEBUG] {message}")
        if logger:
            logger.debug(message)
    else:
        print(f"[{level}] {message}")
        if logger:
            logger.log(getattr(logging, level.upper(), logging.INFO), message)

    # å¼ºåˆ¶åˆ·æ–°è¾“å‡º
    sys.stdout.flush()

# å…¼å®¹æ€§å‡½æ•°
def setup_realtime_logging(log_path: Path, stream_output: bool = True) -> logging.Logger:
    """å…¼å®¹æ€§å‡½æ•°ï¼Œè°ƒç”¨æ–°çš„setup_logging"""
    return setup_logging(log_path, stream_output)

def log_realtime(logger: logging.Logger, message: str, level: str = "INFO"):
    """å…¼å®¹æ€§å‡½æ•°ï¼Œè°ƒç”¨æ–°çš„log_message"""
    log_message(logger, message, level)


def parse_yaml_front_matter(text: str) -> Optional[Dict]:
    """è§£æYAML front matter"""
    if not text.strip().startswith('---'):
        return None
    
    try:
        # æ‰¾åˆ°ç¬¬ä¸€ä¸ªå’Œç¬¬äºŒä¸ª ---
        lines = text.split('\n')
        if len(lines) < 2:
            return None
        
        start_idx = None
        end_idx = None
        
        for i, line in enumerate(lines):
            if line.strip() == '---':
                if start_idx is None:
                    start_idx = i
                else:
                    end_idx = i
                    break
        
        if start_idx is None or end_idx is None:
            return None
        
        # æå–YAMLéƒ¨åˆ†
        yaml_text = '\n'.join(lines[start_idx + 1:end_idx])
        return yaml.safe_load(yaml_text)
    except Exception:
        return None

def get_repetition_config(strict_mode: bool = False) -> dict:
    """
    è·å–é‡å¤æ£€æµ‹é…ç½®
    
    Args:
        strict_mode: æ˜¯å¦å¯ç”¨ä¸¥æ ¼æ¨¡å¼
        
    Returns:
        åŒ…å«é‡å¤æ£€æµ‹å‚æ•°çš„å­—å…¸
    """
    if strict_mode:
        return {
            "max_repeat_chars": 5,      # ä¸¥æ ¼æ¨¡å¼ï¼šå•å­—ç¬¦æœ€å¤šé‡å¤5æ¬¡
            "max_repeat_segments": 3,   # ä¸¥æ ¼æ¨¡å¼ï¼šç‰‡æ®µæœ€å¤šé‡å¤3æ¬¡
            "stream_threshold": 5,      # ä¸¥æ ¼æ¨¡å¼ï¼šæµå¼æ£€æµ‹é˜ˆå€¼5
            "basic_char_threshold": 5,  # ä¸¥æ ¼æ¨¡å¼ï¼šåŸºç¡€æ£€æµ‹å•å­—ç¬¦é˜ˆå€¼5
        }
    else:
        return {
            "max_repeat_chars": 10,     # æ­£å¸¸æ¨¡å¼ï¼šå•å­—ç¬¦æœ€å¤šé‡å¤10æ¬¡
            "max_repeat_segments": 5,   # æ­£å¸¸æ¨¡å¼ï¼šç‰‡æ®µæœ€å¤šé‡å¤5æ¬¡  
            "stream_threshold": 8,      # æ­£å¸¸æ¨¡å¼ï¼šæµå¼æ£€æµ‹é˜ˆå€¼8
            "basic_char_threshold": 8,  # æ­£å¸¸æ¨¡å¼ï¼šåŸºç¡€æ£€æµ‹å•å­—ç¬¦é˜ˆå€¼8
        }

def detect_and_truncate_repetition(text: str, max_repeat_chars: int = 10, max_repeat_segments: int = 5) -> str:
    """
    æ£€æµ‹å¹¶æˆªæ–­é‡å¤æ¨¡å¼ï¼Œé˜²æ­¢æ— é™é‡å¤è¾“å‡º
    
    Args:
        text: è¾“å…¥æ–‡æœ¬
        max_repeat_chars: å•ä¸ªå­—ç¬¦æœ€å¤§è¿ç»­é‡å¤æ¬¡æ•°
        max_repeat_segments: çŸ­ç‰‡æ®µæœ€å¤§é‡å¤æ¬¡æ•°
    
    Returns:
        æˆªæ–­é‡å¤åçš„æ–‡æœ¬
    """
    if not text or len(text) < 10:  # é™ä½æœ€å°é•¿åº¦è¦æ±‚
        return text
    
    # 1. æ£€æµ‹å’Œæˆªæ–­å•å­—ç¬¦é‡å¤
    result = []
    i = 0
    # print(f"    å¼€å§‹æ£€æµ‹å•å­—ç¬¦é‡å¤ï¼Œæ–‡æœ¬é•¿åº¦: {len(text)}")  # å¯é€‰çš„è°ƒè¯•è¾“å‡º
    
    while i < len(text):
        char = text[i]
        count = 1
        j = i + 1
        
        # è®¡ç®—è¿ç»­ç›¸åŒå­—ç¬¦çš„æ•°é‡
        while j < len(text) and text[j] == char:
            count += 1
            j += 1
        
        # å¦‚æœé‡å¤æ¬¡æ•°è¶…è¿‡é˜ˆå€¼ï¼Œæˆªæ–­åˆ°é˜ˆå€¼
        if count > max_repeat_chars:
            result.append(char * max_repeat_chars)
            print(f"    æ£€æµ‹åˆ°å­—ç¬¦ '{char}' é‡å¤ {count} æ¬¡ï¼Œæˆªæ–­åˆ° {max_repeat_chars} æ¬¡")
            # å¦‚æœæ£€æµ‹åˆ°ä¸¥é‡é‡å¤ï¼Œç›´æ¥æˆªæ–­æ•´ä¸ªæ–‡æœ¬åˆ°è¿™é‡Œ
            if count > max_repeat_chars * 3:  # å¦‚æœé‡å¤è¶…è¿‡é˜ˆå€¼çš„3å€ï¼Œå¯èƒ½æ˜¯æ— é™é‡å¤
                print(f"    æ£€æµ‹åˆ°ä¸¥é‡é‡å¤ï¼Œæˆªæ–­æ•´ä¸ªæ–‡æœ¬")
                return ''.join(result)
        else:
            # if count > 1:  # åªåœ¨æœ‰é‡å¤æ—¶æ‰“å° - æ³¨é‡Šæ‰ä»¥å‡å°‘è¾“å‡º
            #     print(f"    å­—ç¬¦ '{char}' é‡å¤ {count} æ¬¡ (æ­£å¸¸èŒƒå›´)")
            result.append(char * count)
        
        i = j
    
    text = ''.join(result)
    
    # 2. æ£€æµ‹å’Œæˆªæ–­çŸ­ç‰‡æ®µé‡å¤
    # ä»æ–‡æœ¬æœ«å°¾å¼€å§‹æ£€æŸ¥ï¼Œå› ä¸ºé‡å¤é€šå¸¸å‡ºç°åœ¨æœ«å°¾
    # print(f"    å¼€å§‹æ£€æµ‹ç‰‡æ®µé‡å¤ï¼Œæ–‡æœ¬é•¿åº¦: {len(text)}")  # å¯é€‰çš„è°ƒè¯•è¾“å‡º
    if len(text) > 20:  # é™ä½æ£€æŸ¥é˜ˆå€¼
        tail = text[-min(1000, len(text)):]  # æ£€æŸ¥æœ€åéƒ¨åˆ†å­—ç¬¦
        
        # æ£€æµ‹ä¸åŒé•¿åº¦çš„é‡å¤ç‰‡æ®µ
        for segment_len in range(5, min(101, len(tail) // 2 + 1), 5):  # 5åˆ°100å­—ç¬¦çš„ç‰‡æ®µ
            if segment_len > len(tail) // 2:
                continue
                
            segment = tail[-segment_len:]
            if not segment.strip():
                continue
                
            # è®¡ç®—è¯¥ç‰‡æ®µåœ¨æ•´ä¸ªå°¾éƒ¨é‡å¤çš„æ¬¡æ•°
            repeat_count = 0
            search_text = tail
            start_pos = 0
            
            # ä½¿ç”¨ç®€å•çš„å­—ç¬¦ä¸²è®¡æ•°æ–¹æ³•
            while True:
                pos = search_text.find(segment, start_pos)
                if pos == -1:
                    break
                repeat_count += 1
                start_pos = pos + segment_len
            
            # å¦‚æœé‡å¤æ¬¡æ•°è¶…è¿‡é˜ˆå€¼ï¼Œæˆªæ–­
            if repeat_count > max_repeat_segments:
                # æ‰¾åˆ°é‡å¤å¼€å§‹çš„ä½ç½®
                repeat_start = len(text) - (repeat_count * segment_len)
                truncated_text = text[:repeat_start + (max_repeat_segments * segment_len)]
                print(f"    æ£€æµ‹åˆ°ç‰‡æ®µé‡å¤ {repeat_count} æ¬¡ï¼ˆé•¿åº¦ {segment_len}ï¼‰ï¼Œæˆªæ–­åˆ° {max_repeat_segments} æ¬¡")
                print(f"    ç‰‡æ®µå†…å®¹: '{segment}'")
                print(f"    åŸæ–‡é•¿åº¦: {len(text)}, æˆªæ–­åé•¿åº¦: {len(truncated_text)}")
                return truncated_text
            # elif repeat_count > 1:  # æ³¨é‡Šæ‰ä»¥å‡å°‘è¾“å‡º
            #     print(f"    å‘ç°ç‰‡æ®µé‡å¤ {repeat_count} æ¬¡ï¼ˆé•¿åº¦ {segment_len}ï¼‰ï¼Œåœ¨æ­£å¸¸èŒƒå›´å†…")
    
    return text

def clean_output_text(text: str) -> str:
    """æ¸…ç†è¾“å‡ºæ–‡æœ¬ï¼Œå»é™¤æ€è€ƒéƒ¨åˆ†ç­‰"""
    if not text or not text.strip():
        return text
    
    # é¦–å…ˆæ£€æµ‹å’Œæˆªæ–­é‡å¤æ¨¡å¼
    text = detect_and_truncate_repetition(text)
    
    # å»é™¤ <think>...</think> éƒ¨åˆ†
    text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL)
    
    # å»é™¤å…¶ä»–å¯èƒ½çš„æ€è€ƒæ ‡è®°
    text = re.sub(r'ï¼ˆæ€è€ƒï¼š.*?ï¼‰', '', text, flags=re.DOTALL)
    text = re.sub(r'ï¼ˆæ³¨ï¼š.*?ï¼‰', '', text, flags=re.DOTALL)
    
    # æ¸…ç†å¤šä½™çš„ç©ºè¡Œï¼Œä½†ä¿ç•™æœ‰æ„ä¹‰çš„ç©ºè¡Œ
    lines = text.split('\n')
    cleaned_lines = []
    for line in lines:
        if line.strip():
            cleaned_lines.append(line)
        elif cleaned_lines and cleaned_lines[-1].strip():  # ä¿ç•™æœ‰æ„ä¹‰çš„ç©ºè¡Œ
            cleaned_lines.append(line)
    
    result = '\n'.join(cleaned_lines).strip()
    return result if result else text  # å¦‚æœæ¸…ç†åä¸ºç©ºï¼Œè¿”å›åŸæ–‡

def check_translation_quality_with_llm(original_text: str, translated_text: str, model: str = "Qwen/Qwen3-32B", bilingual: bool = False) -> Tuple[bool, str]:
    """
    ä½¿ç”¨å¤§æ¨¡å‹æ£€æŸ¥ç¿»è¯‘è´¨é‡ï¼Œç‰¹åˆ«å…³æ³¨æœ€åéƒ¨åˆ†çš„å®Œæ•´æ€§
    æ”¯æŒbilingualå’Œå•è¯­æ¨¡å¼
    è¿”å›: (is_good, reason)
    """
    if not translated_text or not translated_text.strip():
        return False, "ç¿»è¯‘ç»“æœä¸ºç©º"
    
    # æå–åŸæ–‡å’Œç¿»è¯‘çš„æœ€åéƒ¨åˆ†ï¼ˆçº¦800å­—ç¬¦ï¼‰
    original_end = original_text[-800:] if len(original_text) > 800 else original_text
    translated_end = translated_text[-800:] if len(translated_text) > 800 else translated_text
    
    # æ ¹æ®æ¨¡å¼è°ƒæ•´æ£€æµ‹æç¤ºè¯
    if bilingual:
        prompt = f"""æ£€æŸ¥ä»¥ä¸‹æ—¥è¯­åŸæ–‡æœ€åéƒ¨åˆ†å’Œä¸­æ–‡ç¿»è¯‘æœ€åéƒ¨åˆ†ï¼ˆbilingualå¯¹ç…§æ¨¡å¼ï¼‰ï¼Œåˆ¤æ–­ç¿»è¯‘æ˜¯å¦å®Œæ•´æ­£ç¡®ã€‚

åŸæ–‡æœ€åéƒ¨åˆ†ï¼š
{original_end}

ç¿»è¯‘æœ€åéƒ¨åˆ†ï¼ˆbilingualæ ¼å¼ï¼‰ï¼š
{translated_end}

åˆ¤æ–­æ ‡å‡†ï¼š
1. ç¿»è¯‘æ˜¯å¦å®Œæ•´ï¼ˆæ²¡æœ‰é—æ¼åŸæ–‡å†…å®¹ï¼‰
2. ç¿»è¯‘æ˜¯å¦å‡†ç¡®ï¼ˆæ²¡æœ‰é”™è¯¯ï¼‰
3. æ˜¯å¦æ­£å¸¸ç»“æŸï¼ˆæ²¡æœ‰"ä»¥ä¸‹çœç•¥"ç­‰æ ‡è®°ï¼‰
4. bilingualæ ¼å¼æ˜¯å¦æ­£ç¡®ï¼ˆæ—¥è¯­åŸæ–‡åè·Ÿä¸­æ–‡è¯‘æ–‡ï¼‰

å¦‚æœç¿»è¯‘å®Œæ•´æ­£ç¡®ï¼Œå›å¤ï¼šGOOD
å¦‚æœæœ‰é—®é¢˜ï¼Œå›å¤ï¼šBAD

åªå›å¤GOODæˆ–BADã€‚"""
    else:
        prompt = f"""æ£€æŸ¥ä»¥ä¸‹æ—¥è¯­åŸæ–‡æœ€åéƒ¨åˆ†å’Œä¸­æ–‡ç¿»è¯‘æœ€åéƒ¨åˆ†ï¼Œåˆ¤æ–­ç¿»è¯‘æ˜¯å¦å®Œæ•´æ­£ç¡®ã€‚

åŸæ–‡æœ€åéƒ¨åˆ†ï¼š
{original_end}

ç¿»è¯‘æœ€åéƒ¨åˆ†ï¼š
{translated_end}

åˆ¤æ–­æ ‡å‡†ï¼š
1. ç¿»è¯‘æ˜¯å¦å®Œæ•´ï¼ˆæ²¡æœ‰é—æ¼åŸæ–‡å†…å®¹ï¼‰
2. ç¿»è¯‘æ˜¯å¦å‡†ç¡®ï¼ˆæ²¡æœ‰é”™è¯¯ï¼‰
3. æ˜¯å¦æ­£å¸¸ç»“æŸï¼ˆæ²¡æœ‰"ä»¥ä¸‹çœç•¥"ç­‰æ ‡è®°ï¼‰

å¦‚æœç¿»è¯‘å®Œæ•´æ­£ç¡®ï¼Œå›å¤ï¼šGOOD
å¦‚æœæœ‰é—®é¢˜ï¼Œå›å¤ï¼šBAD

åªå›å¤GOODæˆ–BADã€‚"""

    try:
        client = OpenAI(base_url="http://localhost:8000/v1", api_key="dummy")
        resp = client.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.0,
            max_tokens=200,
        )
        result = resp.choices[0].message.content.strip()
        
        if result.upper().startswith("GOOD"):
            mode_text = "bilingualå¯¹ç…§æ¨¡å¼" if bilingual else "å•è¯­æ¨¡å¼"
            return True, f"å¤§æ¨¡å‹è¯„ä¼°ï¼š{mode_text}æœ€åéƒ¨åˆ†ç¿»è¯‘è´¨é‡è‰¯å¥½"
        elif result.upper().startswith("BAD"):
            reason = result[3:].strip() if len(result) > 3 else "å¤§æ¨¡å‹è¯„ä¼°ï¼šæœ€åéƒ¨åˆ†ç¿»è¯‘è´¨é‡ä¸ä½³"
            return False, reason
        else:
            # å¦‚æœæ¨¡å‹å›å¤ä¸æ˜ç¡®ï¼Œä½¿ç”¨åŸºç¡€æ£€æµ‹
            return check_translation_quality_basic(original_text, translated_text, bilingual)
            
    except Exception as e:
        print(f"    å¤§æ¨¡å‹æ£€æµ‹å¤±è´¥: {e}ï¼Œä½¿ç”¨åŸºç¡€æ£€æµ‹")
        return check_translation_quality_basic(original_text, translated_text, bilingual)

def check_translation_quality_basic(original_text: str, translated_text: str, bilingual: bool = False) -> Tuple[bool, str]:
    """
    åŸºç¡€ç¿»è¯‘è´¨é‡æ£€æµ‹ï¼ˆä½œä¸ºå¤§æ¨¡å‹æ£€æµ‹çš„å¤‡é€‰ï¼‰
    """
    if not translated_text or not translated_text.strip():
        return False, "ç¿»è¯‘ç»“æœä¸ºç©º"
    
    # 1. æ£€æŸ¥é•¿åº¦æ¯”ä¾‹ï¼ˆç¿»è¯‘ç»“æœåº”è¯¥è‡³å°‘æ˜¯åŸæ–‡çš„20%ï¼‰
    original_len = len(original_text.strip())
    translated_len = len(translated_text.strip())
    
    if translated_len < original_len * 0.2:
        return False, f"ç¿»è¯‘ç»“æœå¤ªçŸ­: {translated_len}/{original_len} ({translated_len/original_len:.1%})"
    
    # 2. æ£€æŸ¥æ˜¯å¦åŒ…å«æ˜æ˜¾çš„ç¿»è¯‘é”™è¯¯
    error_patterns = [
        r'ï¼ˆä»¥ä¸‹çœç•¥ï¼‰',  # çœç•¥æ ‡è®°
        r'\[TO BE CONTINUED\]',  # æœªå®Œå¾…ç»­
        r'\[\.\.\.\]',  # çœç•¥å·
        r'ï¼ˆæ­¤å¤„çœç•¥',  # çœç•¥è¯´æ˜
        r'ï¼ˆæ³¨ï¼š',  # æ³¨é‡Š
        r'å®Œæ•´ç‰ˆè¯·å‚è€ƒ',  # å®Œæ•´ç‰ˆå¼•ç”¨
        r'ç”±äºæ–‡æœ¬é•¿åº¦é™åˆ¶',  # é•¿åº¦é™åˆ¶è¯´æ˜
        r'å†…å®¹æ€§è´¨åŸå› ',  # å†…å®¹åŸå› 
        r'ä»…å±•ç¤ºéƒ¨åˆ†',  # éƒ¨åˆ†å±•ç¤º
        r'çœç•¥å¤§é‡é‡å¤',  # é‡å¤çœç•¥
        r'æœ€ç»ˆæ®µè½',  # æœ€ç»ˆæ®µè½
        r'ï¼ˆç¿»è¯‘ç»“æŸï¼‰',  # ç¿»è¯‘ç»“æŸ
        r'<think>',  # æ€è€ƒæ ‡è®°
        r'</think>',  # æ€è€ƒæ ‡è®°ç»“æŸ
    ]
    
    for pattern in error_patterns:
        if re.search(pattern, translated_text, re.IGNORECASE):
            return False, f"åŒ…å«é”™è¯¯æ¨¡å¼: {pattern}"
    
    # 3. æ£€æŸ¥æ˜¯å¦åŒ…å«å¤§é‡æ—¥è¯­åŸæ–‡ï¼ˆbilingualæ¨¡å¼ä¸‹æ”¾å®½æ ‡å‡†ï¼‰
    japanese_chars = len(re.findall(r'[\u3040-\u309f\u30a0-\u30ff]', translated_text))
    total_chars = len(translated_text)
    
    # bilingualæ¨¡å¼ä¸‹ï¼Œç”±äºåŒ…å«åŸæ–‡ï¼Œæ—¥è¯­æ¯”ä¾‹ä¼šæ›´é«˜ï¼Œæ”¾å®½åˆ°50%
    max_japanese_ratio = 0.5 if bilingual else 0.3
    
    if japanese_chars > total_chars * max_japanese_ratio:
        return False, f"åŒ…å«è¿‡å¤šæ—¥è¯­åŸæ–‡: {japanese_chars}/{total_chars} ({japanese_chars/total_chars:.1%})"
    
    # 4. æ£€æŸ¥æ˜¯å¦åŒ…å«å¤§é‡é‡å¤å­—ç¬¦ï¼ˆå¦‚é—®å·ã€æ„Ÿå¹å·ç­‰ï¼‰
    # æ£€æŸ¥æœ€å100ä¸ªå­—ç¬¦ä¸­æ˜¯å¦æœ‰è¶…è¿‡50%çš„é‡å¤å­—ç¬¦
    last_100_chars = translated_text.strip()[-100:] if len(translated_text.strip()) >= 100 else translated_text.strip()
    if len(last_100_chars) >= 50:
        char_counts = {}
        for char in last_100_chars:
            char_counts[char] = char_counts.get(char, 0) + 1
        
        # æ‰¾å‡ºå‡ºç°æœ€å¤šçš„å­—ç¬¦
        most_common_char = max(char_counts.items(), key=lambda x: x[1])
        if most_common_char[1] > len(last_100_chars) * 0.5:  # è¶…è¿‡50%æ˜¯åŒä¸€ä¸ªå­—ç¬¦
            return False, f"ç»“å°¾åŒ…å«å¤§é‡é‡å¤å­—ç¬¦: {most_common_char[0]} ({most_common_char[1]}/{len(last_100_chars)})"
    
    # 5. æ£€æŸ¥æ˜¯å¦ä»¥ä¸å®Œæ•´çš„å¥å­ç»“å°¾
    if not translated_text.strip().endswith(('ã€‚', 'ï¼', 'ï¼Ÿ', 'â€¦', '"', '"', ''', ''', 'ï¼‰', 'ã€‘', 'ã€', 'ã€')):
        last_sentence = translated_text.strip().split('\n')[-1]
        if len(last_sentence) > 20 and not any(last_sentence.endswith(end) for end in ('ã€‚', 'ï¼', 'ï¼Ÿ', 'â€¦', '"', '"', ''', ''', 'ï¼‰', 'ã€‘', 'ã€', 'ã€')):
            return False, "å¥å­ä¸å®Œæ•´ç»“å°¾"
    
    return True, "åŸºç¡€æ£€æµ‹ï¼šç¿»è¯‘è´¨é‡è‰¯å¥½"

def clean_output_text(text: str) -> str:
    if not text or not text.strip():
        return text
    
    # å»é™¤ <think>...</think> éƒ¨åˆ†
    import re
    text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL)
    
    # å»é™¤å…¶ä»–å¯èƒ½çš„æ€è€ƒæ ‡è®°
    text = re.sub(r'ï¼ˆæ€è€ƒï¼š.*?ï¼‰', '', text, flags=re.DOTALL)
    text = re.sub(r'ï¼ˆæ³¨ï¼š.*?ï¼‰', '', text, flags=re.DOTALL)
    
    # æ¸…ç†å¤šä½™çš„ç©ºè¡Œï¼Œä½†ä¿ç•™æœ‰æ„ä¹‰çš„ç©ºè¡Œ
    lines = text.split('\n')
    cleaned_lines = []
    for line in lines:
        if line.strip():
            cleaned_lines.append(line)
        elif cleaned_lines and cleaned_lines[-1].strip():  # ä¿ç•™æœ‰æ„ä¹‰çš„ç©ºè¡Œ
            cleaned_lines.append(line)
    
    result = '\n'.join(cleaned_lines).strip()
    return result if result else text  # å¦‚æœæ¸…ç†åä¸ºç©ºï¼Œè¿”å›åŸæ–‡

def split_text_by_lines(text: str, max_chars: int, overlap_chars: int) -> List[str]:
    """æŒ‰è¡Œåˆ†å‰²æ–‡æœ¬ï¼Œç¡®ä¿æ•´è¡Œä¸è¢«æˆªæ–­"""
    lines = text.split('\n')
    chunks = []
    current_chunk = []
    current_length = 0
    
    for line in lines:
        line_length = len(line) + 1  # +1 for newline
        
        # å¦‚æœå½“å‰è¡ŒåŠ ä¸Šå½“å‰å—ä¼šè¶…å‡ºé™åˆ¶
        if current_length + line_length > max_chars and current_chunk:
            # ä¿å­˜å½“å‰å—
            chunk_text = '\n'.join(current_chunk)
            chunks.append(chunk_text)
            
            # è®¡ç®—é‡å éƒ¨åˆ†ï¼ˆä»å½“å‰å—çš„æœ«å°¾å–overlap_charså­—ç¬¦ï¼‰
            if overlap_chars > 0 and len(chunk_text) > overlap_chars:
                overlap_text = chunk_text[-overlap_chars:]
                # æ‰¾åˆ°é‡å æ–‡æœ¬ä¸­æœ€åä¸€ä¸ªå®Œæ•´çš„è¡Œ
                last_newline = overlap_text.rfind('\n')
                if last_newline > 0:
                    overlap_text = overlap_text[last_newline + 1:]
                current_chunk = [overlap_text] if overlap_text else []
                current_length = len(overlap_text)
            else:
                current_chunk = []
                current_length = 0
        
        # æ·»åŠ å½“å‰è¡Œåˆ°å—ä¸­
        current_chunk.append(line)
        current_length += line_length
    
    # æ·»åŠ æœ€åä¸€ä¸ªå—
    if current_chunk:
        chunks.append('\n'.join(current_chunk))
    
    return chunks


def load_few_shot_samples(sample_file: Path) -> List[Tuple[str, str]]:
    """ä» sample.txt åŠ è½½ few-shot ç¤ºä¾‹"""
    if not sample_file.exists():
        return []
    
    try:
        content = sample_file.read_text(encoding="utf-8")
        samples = []
        lines = content.split('\n')
        i = 0
        while i < len(lines):
            if lines[i].strip() == 'input:':
                # æ‰¾åˆ° input å¼€å§‹
                input_lines = []
                i += 1
                while i < len(lines) and lines[i].strip() != 'output:':
                    input_lines.append(lines[i])
                    i += 1
                
                if i < len(lines) and lines[i].strip() == 'output:':
                    # æ‰¾åˆ° output å¼€å§‹
                    output_lines = []
                    i += 1
                    while i < len(lines) and (lines[i].strip() or not lines[i].strip().startswith('input:')):
                        if lines[i].strip().startswith('input:'):
                            break
                        output_lines.append(lines[i])
                        i += 1
                    
                    if input_lines and output_lines:
                        input_text = '\n'.join(input_lines).strip()
                        output_text = '\n'.join(output_lines).strip()
                        samples.append((input_text, output_text))
                else:
                    i += 1
            else:
                i += 1
        
        return samples
    except Exception as e:
        print(f"WARN: æ— æ³•åŠ è½½ few-shot ç¤ºä¾‹: {e}")
        return []


def split_yaml_and_body(text: str) -> Tuple[str, str]:
    s = text.lstrip()
    if not s.startswith("---\n"):
        return "", text
    # æ‰¾åˆ°ç¬¬äºŒä¸ª '---' è¡Œ
    parts = s.split("\n---\n", 1)
    if len(parts) == 2:
        yaml_part = parts[0][4:]  # å»æ‰å¼€å¤´çš„ '---\n'
        body = parts[1]
        return yaml_part, body.lstrip("\n")
    return "", text


def parse_yaml_minimal(yaml_text: str) -> Dict[str, str]:
    meta: Dict[str, str] = {}
    for line in yaml_text.splitlines():
        if not line.strip():
            continue
        if ":" not in line:
            continue
        try:
            k, v = line.split(":", 1)
            meta[k.strip()] = v.strip()
        except ValueError:
            continue
    return meta


def split_paragraphs(body: str) -> List[str]:
    lines = body.replace("\r\n", "\n").replace("\r", "\n").split("\n")
    paras: List[str] = []
    buf: List[str] = []
    for ln in lines:
        if ln.strip() == "":
            if buf:
                paras.append("\n".join(buf).strip())
                buf = []
        else:
            buf.append(ln)
    if buf:
        paras.append("\n".join(buf).strip())
    return paras


def translate_with_local_llm(text: str, model: str, temperature: float, max_tokens: int, terminology: Optional[str] = None, stop: Optional[List[str]] = None, frequency_penalty: Optional[float] = None, presence_penalty: Optional[float] = None, few_shot_samples: Optional[List[Tuple[str, str]]] = None, max_context_length: Optional[int] = None, preface_file: Optional[str] = None, bilingual: bool = False, stream: bool = False, logger: Optional[logging.Logger] = None) -> Tuple[str, str, Dict[str, int]]:
    # ç»„è£…å¸¦æœ¯è¯­è¡¨çš„æç¤ºè¯
    if preface_file and Path(preface_file).exists():
        with open(preface_file, 'r', encoding='utf-8') as f:
            preface = f.read().strip() + "\n"
    else:
        # å¦‚æœæ²¡æœ‰æä¾›preface_fileï¼Œä½¿ç”¨é»˜è®¤çš„ç¿»è¯‘æŒ‡ä»¤
        if bilingual:
            preface = """è¯·å°†ä»¥ä¸‹æ—¥è¯­æ–‡æœ¬å¿ å®ç¿»è¯‘ä¸ºä¸­æ–‡ï¼Œå¹¶æŒ‰ç…§ç¤ºä¾‹æ ¼å¼è¾“å‡ºåŒè¯­å¯¹ç…§æ ¼å¼ï¼š
- è¾“å‡ºæ ¼å¼ï¼šè¯·ä¸¥æ ¼æŒ‰ç…§ç¤ºä¾‹æ ¼å¼ï¼Œé€è¡Œè¾“å‡ºæ—¥è¯­åŸæ–‡+ä¸­æ–‡è¯‘æ–‡å¯¹ç…§æ ¼å¼ï¼Œå³æ¯è¡Œæ—¥è¯­åŸæ–‡åç´§è·Ÿå¯¹åº”çš„ä¸­æ–‡è¯‘æ–‡ï¼›
- ä¸¥æ ¼ä¿æŒåŸæ–‡çš„åˆ†æ®µä¸åˆ†è¡Œï¼Œä¸åˆå¹¶ã€ä¸çœç•¥ã€ä¸æ·»åŠ è§£é‡Šï¼›
- å¯¹è¯ä¸å¼•å·æ ·å¼å¯¹é½ï¼Œç©ºè¡Œä½ç½®ä¿æŒä¸å˜ï¼›
- æ‹Ÿå£°è¯ç¿»è¯‘è§„åˆ™ï¼šå°†æ—¥è¯­æ‹Ÿå£°è¯ç¿»è¯‘ä¸ºå¯¹åº”çš„ä¸­æ–‡æ‹Ÿå£°è¯ï¼Œå¦‚ã€Œã©ã³ã‚…ã³ã‚…ã£ã€â†’ã€Œå™—å‘²å‘²ã€ï¼›è‹¥é‡åˆ°éš¾ä»¥å¯¹åº”åˆ°ä¸­æ–‡å‘éŸ³çš„å­¤ç«‹éŸ³èŠ‚ï¼ˆå¦‚ã€Œã£ã€ã€Œã‚“ã€ç­‰ï¼‰ï¼Œå¯é€‚å½“çœç•¥ï¼Œä»…ä¿ç•™èƒ½å¯¹åº”çš„éƒ¨åˆ†ï¼›
- æ–­ç‚¹è¯å¤„ç†ï¼šå¯¹äºã€Œã›ãƒ»ã‚“ãƒ»ã›ã€è¿™ç§å¸¦æ–­ç‚¹çš„è¯ï¼Œå…ˆç¿»è¯‘å®Œæ•´è¯ï¼ˆå¦‚ã€Œå…ˆç”Ÿã€â†’ã€Œè€å¸ˆã€ï¼‰ï¼Œç„¶ååœ¨ä¸­æ–‡è¯ä¸Šæ·»åŠ æ–­ç‚¹ï¼ˆå¦‚ã€Œè€ãƒ»å¸ˆã€ï¼‰ï¼›
- ä»…è¾“å‡ºåŒè¯­å¯¹ç…§æ ¼å¼çš„ç¿»è¯‘ç»“æœï¼Œä¸è¦é¢å¤–è¯´æ˜æˆ–æ€è€ƒå†…å®¹ã€‚\n\n"""
        else:
            preface = """è¯·å°†ä»¥ä¸‹æ—¥è¯­æ–‡æœ¬å¿ å®ç¿»è¯‘ä¸ºä¸­æ–‡ï¼š
- ä¸¥æ ¼ä¿æŒåŸæ–‡çš„åˆ†æ®µä¸åˆ†è¡Œï¼Œä¸åˆå¹¶ã€ä¸çœç•¥ã€ä¸æ·»åŠ è§£é‡Šï¼›
- å¯¹è¯ä¸å¼•å·æ ·å¼å¯¹é½ï¼Œç©ºè¡Œä½ç½®ä¿æŒä¸å˜ï¼›
- æ‹Ÿå£°è¯ç¿»è¯‘è§„åˆ™ï¼šå°†æ—¥è¯­æ‹Ÿå£°è¯ç¿»è¯‘ä¸ºå¯¹åº”çš„ä¸­æ–‡æ‹Ÿå£°è¯ï¼›
- æ–­ç‚¹è¯å¤„ç†ï¼šå¯¹äºå¸¦æ–­ç‚¹çš„è¯ï¼Œå…ˆç¿»è¯‘å®Œæ•´è¯ï¼Œç„¶ååœ¨ä¸­æ–‡è¯ä¸Šæ·»åŠ æ–­ç‚¹ï¼›
- ä»…è¾“å‡ºç¿»è¯‘ç»“æœï¼Œä¸è¦é¢å¤–è¯´æ˜æˆ–æ€è€ƒå†…å®¹ã€‚\n\n"""
    if terminology:
        preface += "ä»¥ä¸‹æ˜¯æœ¯è¯­å¯¹ç…§è¡¨ï¼Œè¯·ä¸¥æ ¼å‚ç…§ï¼š\n" + terminology.strip() + "\n\n"
    
    prompt = preface
    if few_shot_samples:
        prompt += "\n\nFew-shot ç¤ºä¾‹ï¼š\n"
        for i, (input_text, output_text) in enumerate(few_shot_samples, 1):
            prompt += f"ç¤ºä¾‹ {i}:\nè¾“å…¥:\n{input_text}\n\nè¾“å‡º:\n{output_text}\n\n"
        prompt += "è¯·æ ¹æ®è¿™äº›ç¤ºä¾‹ï¼Œå¿ å®åœ°ç¿»è¯‘ä»¥ä¸‹æ–‡æœ¬ã€‚\n\n"
    
    prompt += "åŸæ–‡ï¼š\n\n" + text + "\n\nç¿»è¯‘ç»“æœï¼š"
    
    # ä¼°ç®—è¾“å…¥tokensï¼ˆç²—ç•¥ï¼‰ï¼šæŒ‰å­—ç¬¦æ•° * 0.7
    estimated_input_tokens = int(len(prompt) * 0.7)
    log_message(logger, f"è°ƒç”¨æ¨¡å‹ï¼Œprompté•¿åº¦: {len(prompt)}")
    log_message(logger, f"å®Œæ•´prompt:\n{prompt}")
    client = OpenAI(base_url="http://localhost:8000/v1", api_key="dummy")
    kwargs = {}
    if stop:
        kwargs["stop"] = stop
    if frequency_penalty is not None:
        kwargs["frequency_penalty"] = frequency_penalty
    if presence_penalty is not None:
        kwargs["presence_penalty"] = presence_penalty
    # vLLM ç«¯é€šå¸¸è¦æ±‚æ˜¾å¼ max_tokensï¼›å½“ <=0 æ—¶ä½¿ç”¨å®‰å…¨å¤§å€¼
    chosen_max_tokens = max_tokens
    if not isinstance(chosen_max_tokens, int) or chosen_max_tokens <= 0:
        # åŠ¨æ€è®¡ç®—åˆé€‚çš„ max_tokensï¼Œç¡®ä¿ä¸è¶…è¿‡æ¨¡å‹çš„æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦
        # å¦‚æœæ²¡æœ‰ä¼ å…¥max_context_lengthï¼Œåˆ™æ ¹æ®æ¨¡å‹åç§°æ¨æ–­é»˜è®¤å€¼
        if max_context_length is None:
            if "32B" in model and "AWQ" not in model:
                # å®Œæ•´32Bæ¨¡å‹ï¼š32768 tokens
                max_context_length = 32768
            else:
                # AWQæˆ–å…¶ä»–æ¨¡å‹ï¼š40960 tokens
                max_context_length = 40960
        
        # ä¸ºäº†å®‰å…¨èµ·è§ï¼Œæˆ‘ä»¬ä½¿ç”¨æ›´ä¿å®ˆçš„ä¼°ç®—
        # é¢„ç•™ 2000 tokens ä½œä¸ºå®‰å…¨è¾¹ç•Œ
        safe_max_tokens = max_context_length - int(estimated_input_tokens) - 2000
        chosen_max_tokens = max(1000, safe_max_tokens)  # è‡³å°‘ä¿ç•™ 1000 tokens
        print(f"    åŠ¨æ€è®¡ç®— max_tokens: {chosen_max_tokens} (åŸºäºè¾“å…¥é•¿åº¦ {len(prompt)}, ä¼°ç®—è¾“å…¥tokens: {estimated_input_tokens}, æ¨¡å‹ä¸Šä¸‹æ–‡é•¿åº¦: {max_context_length})")
    # ä¼°è®¡è¾“å‡ºtokensä¸Šé™ï¼ˆå«æ€è€ƒ+è¯‘æ–‡ï¼‰ï¼šå– chosen_max_tokens
    token_meta: Dict[str, int] = {
        "estimated_input_tokens": int(estimated_input_tokens),
        "estimated_output_tokens": int(max(0, chosen_max_tokens)),
        "max_context_length": int(max_context_length if max_context_length else 0),
        "used_max_tokens": int(chosen_max_tokens),
        "prompt_chars": len(prompt),
        "text_chars": len(text),
    }
    try:
        if stream:
            log_message(logger, f"å¼€å§‹æµå¼è°ƒç”¨...")
            result = ""
            resp = client.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": prompt}],
                temperature=temperature,
                max_tokens=chosen_max_tokens,
                stream=True,
                **kwargs,
            )
            
            # æµå¼é‡å¤æ£€æµ‹å‚æ•°ï¼ˆæ ¹æ®ä¸¥æ ¼æ¨¡å¼è°ƒæ•´ï¼‰
            repetition_buffer = ""
            max_buffer_size = 500  # ä¿ç•™æœ€è¿‘500å­—ç¬¦ç”¨äºæ£€æµ‹
            # æ ¹æ®æ¨¡å‹å‚æ•°ä¸­çš„ä¸¥æ ¼æ¨¡å¼è°ƒæ•´é˜ˆå€¼
            # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬éœ€è¦ä»å¤–å±‚ä¼ é€’strict_modeå‚æ•°ï¼Œæš‚æ—¶ä½¿ç”¨é»˜è®¤å€¼
            repetition_threshold = 8  # è¿ç»­é‡å¤é˜ˆå€¼ï¼Œæ›´ä¸¥æ ¼
            
            for chunk in resp:
                if chunk.choices[0].delta.content:
                    content = chunk.choices[0].delta.content
                    
                    # æ£€æµ‹å®æ—¶é‡å¤
                    temp_result = result + content
                    repetition_buffer += content
                    
                    # ä¿æŒbufferå¤§å°
                    if len(repetition_buffer) > max_buffer_size:
                        repetition_buffer = repetition_buffer[-max_buffer_size:]
                    
                    # æ£€æµ‹å•å­—ç¬¦é‡å¤
                    should_stop = False
                    if len(repetition_buffer) >= repetition_threshold:
                        # æ£€æŸ¥æœ€åçš„å­—ç¬¦æ˜¯å¦é‡å¤è¿‡å¤š
                        last_char = repetition_buffer[-1]
                        consecutive_count = 0
                        for i in range(len(repetition_buffer) - 1, -1, -1):
                            if repetition_buffer[i] == last_char:
                                consecutive_count += 1
                            else:
                                break
                        
                        if consecutive_count >= repetition_threshold:
                            print(f"\n[æ£€æµ‹åˆ°é‡å¤è¾“å‡ºï¼Œåœæ­¢ç”Ÿæˆ] å­—ç¬¦ '{last_char}' è¿ç»­é‡å¤ {consecutive_count} æ¬¡")
                            should_stop = True
                    
                    if should_stop:
                        break
                    
                    result += content
                    # æµå¼è¾“å‡ºåªåœ¨æ§åˆ¶å°æ˜¾ç¤ºï¼Œä¸è®°å½•åˆ°æ—¥å¿—æ–‡ä»¶
                    print(content, end="", flush=True)
            print()  # æ¢è¡Œ
            result = result.strip()
            # åœ¨æµå¼è¾“å‡ºå®Œæˆåï¼Œåœ¨æ§åˆ¶å°æ˜¾ç¤ºå®Œæ•´çš„ç¿»è¯‘ç»“æœ
            log_message(logger, f"ç¿»è¯‘å®Œæˆï¼Œç»“æœé•¿åº¦: {len(result)}")
            # åœ¨æ§åˆ¶å°æ˜¾ç¤ºå‰å‡ è¡Œä½œä¸ºé¢„è§ˆ
            lines = result.split('\n')
            preview_lines = lines[:10]  # æ˜¾ç¤ºå‰10è¡Œ
            log_message(logger, f"ç¿»è¯‘ç»“æœé¢„è§ˆï¼ˆå‰10è¡Œï¼‰:")
            for line in preview_lines:
                log_message(logger, f"    {line}")
            if len(lines) > 10:
                log_message(logger, f"    ... (è¿˜æœ‰ {len(lines) - 10} è¡Œ)")
        else:
            resp = client.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": prompt}],
                temperature=temperature,
                max_tokens=chosen_max_tokens,
                **kwargs,
            )
            result = resp.choices[0].message.content.strip()
        
        log_message(logger, f"æ¨¡å‹è¿”å›ï¼Œç»“æœé•¿åº¦: {len(result)}")
        
        # è®¡ç®—å®é™…ä½¿ç”¨çš„tokenæ•°
        actual_output_tokens = int(len(result) * 0.7)  # ç²—ç•¥ä¼°ç®—
        total_tokens = estimated_input_tokens + actual_output_tokens
        
        # æ›´æ–°token_meta
        token_meta.update({
            "actual_output_tokens": actual_output_tokens,
            "total_tokens": total_tokens,
            "result_chars": len(result),
            "used_max_tokens": total_tokens,  # ä¿®æ­£ï¼šå®é™…ä½¿ç”¨çš„max_tokensåº”è¯¥æ˜¯æ€»tokens
        })
        
        return result, prompt, token_meta
    except Exception as e:
        log_message(logger, f"æ¨¡å‹è°ƒç”¨å¤±è´¥: {e}", "ERROR")
        raise


def looks_bad_output(text: str, original_text: str = "") -> bool:
    if not text:
        print("    looks_bad_output: æ–‡æœ¬ä¸ºç©º")
        return True
    
    print(f"    looks_bad_output: æ£€æŸ¥æ–‡æœ¬ï¼Œé•¿åº¦={len(text)}")
    
    # æ£€æµ‹é‡å¤æ¨¡å¼ï¼ˆæ›´å®½æ¾ï¼‰
    tail = text[-1000:]  # æ£€æŸ¥å°¾éƒ¨1000å­—ç¬¦ï¼ˆä¹‹å‰æ˜¯800ï¼‰
    
    # 1. æ£€æµ‹è¿ç»­é‡å¤çš„çŸ­ç‰‡æ®µï¼ˆé•¿åº¦10-100é‡å¤3æ¬¡ä»¥ä¸Šï¼Œä¹‹å‰æ˜¯5-50é‡å¤2æ¬¡ï¼‰
    for w in range(10, 101, 10):
        if w > len(tail):
            continue
        seg = tail[-w:]
        if seg and tail.count(seg) >= 3:  # ä»2æ¬¡æ”¹ä¸º3æ¬¡
            print(f"    looks_bad_output: æ£€æµ‹åˆ°é‡å¤ç‰‡æ®µï¼Œé•¿åº¦={w}")
            return True
    
    # 2. æ£€æµ‹å•å­—ç¬¦è¿‡é•¿é‡å¤ï¼ˆè¶…è¿‡8æ¬¡ï¼Œæ¢å¤æ›´ä¸¥æ ¼çš„æ£€æµ‹ï¼‰
    for ch in set(tail):
        if ch * 8 in tail:  # æ¢å¤ä¸º8æ¬¡ï¼Œæ›´ä¸¥æ ¼æ£€æµ‹
            print(f"    looks_bad_output: æ£€æµ‹åˆ°å•å­—ç¬¦é‡å¤: {ch}")
            return True
    
    # 3. æ£€æµ‹å¼‚å¸¸ç»“å°¾æ¨¡å¼ï¼ˆä¿æŒä¸å˜ï¼‰
    bad_endings = [
        "æœªå®Œå¾…ç»­", "[TO BE CONTINUED]", "[...]", "ï¼ˆæ­¤å¤„çœç•¥", "ï¼ˆæ³¨ï¼š", 
        "å®Œæ•´ç‰ˆè¯·å‚è€ƒ", "ç”±äºæ–‡æœ¬é•¿åº¦é™åˆ¶", "å†…å®¹æ€§è´¨åŸå› ", "ä»…å±•ç¤ºéƒ¨åˆ†",
        "çœç•¥å¤§é‡é‡å¤", "æœ€ç»ˆæ®µè½", "---", "###", "***", "ï¼ˆç¿»è¯‘ç»“æŸï¼‰"
    ]
    if any(bad in tail for bad in bad_endings):
        print(f"    looks_bad_output: æ£€æµ‹åˆ°å¼‚å¸¸ç»“å°¾")
        return True
    
    # 4. æ£€æµ‹éä¸­æ–‡/æ—¥æ–‡å­—ç¬¦è¿‡å¤šï¼ˆé˜ˆå€¼ä»30%æ”¹ä¸º50%ï¼‰
    non_cjk = sum(1 for c in tail if not ('\u4e00' <= c <= '\u9fff' or '\u3040' <= c <= '\u309f' or '\u30a0' <= c <= '\u30ff' or c in 'ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š""''ï¼ˆï¼‰ã€ã€‘â€¦â€”'))
    if non_cjk > len(tail) * 0.5:  # ä»0.3æ”¹ä¸º0.5
        print(f"    looks_bad_output: éCJKå­—ç¬¦è¿‡å¤š: {non_cjk}/{len(tail)}")
        return True
    
    # 5. æ£€æµ‹æ ‡ç‚¹ç¬¦å·ç¼ºå¤±ï¼ˆé˜ˆå€¼ä»50æ”¹ä¸º100ï¼‰
    cjk_only = ''.join(c for c in tail if '\u4e00' <= c <= '\u9fff')
    if len(cjk_only) > 100 and not any(p in tail for p in 'ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š'):  # ä»50æ”¹ä¸º100
        print(f"    looks_bad_output: æ ‡ç‚¹ç¬¦å·ç¼ºå¤±")
        return True
    
    # 6. æ£€æµ‹ç¿»è¯‘å®Œæ•´æ€§ï¼ˆé˜ˆå€¼ä»30%æ”¹ä¸º20%ï¼‰
    if original_text:
        original_len = len(original_text.strip())
        translated_len = len(text.strip())
        
        if translated_len < original_len * 0.2:  # ä»0.3æ”¹ä¸º0.2
            print(f"    looks_bad_output: è¯‘æ–‡å¤ªçŸ­: {translated_len}/{original_len}")
            return True
        
        # å¦‚æœè¯‘æ–‡çªç„¶ç»“æŸï¼Œæ²¡æœ‰æ˜æ˜¾çš„ç»“å°¾æ ‡è®°
        if not text.strip().endswith(('ã€‚', 'ï¼', 'ï¼Ÿ', 'â€¦', '"', '"', ''', ''', 'ï¼‰', 'ã€‘')):
            last_sentence = text.strip().split('\n')[-1]
            if len(last_sentence) > 20 and not any(last_sentence.endswith(end) for end in ('ã€‚', 'ï¼', 'ï¼Ÿ', 'â€¦', '"', '"', ''', ''', 'ï¼‰', 'ã€‘')):  # ä»10æ”¹ä¸º20
                print(f"    looks_bad_output: å¥å­çªç„¶ç»“æŸ")
                return True
    
    print("    looks_bad_output: æ–‡æœ¬æ£€æŸ¥é€šè¿‡")
    return False


def translate_chunk_with_retry(
    chunk_text: str,
    model: str,
    temperature: float,
    max_tokens: int,
    terminology_txt: Optional[str],
    stop: Optional[List[str]],
    frequency_penalty: Optional[float],
    presence_penalty: Optional[float],
    retries: int,
    retry_wait: float,
    few_shot_samples: Optional[List[Tuple[str, str]]],
    max_context_length: Optional[int] = None,
    preface_file: Optional[str] = None,
    bilingual: bool = False,
    stream: bool = False,
    logger: Optional[logging.Logger] = None,
    chunk_index: Optional[int] = None,
) -> Tuple[str, str, bool, Dict[str, int]]:
    """è¿”å› (output, prompt, ok)ã€‚ok=False è¡¨ç¤ºå»ºè®®é™çº§åˆ†å—/æˆ–é‡è¯•å¤±è´¥ã€‚"""
    last_err = None
    # é¢„å…ˆè®¡ç®—token_metaï¼Œä»¥ä¾¿åœ¨å¤±è´¥æ—¶ä¹Ÿèƒ½è¿”å›æœ‰æ•ˆä¿¡æ¯
    estimated_input_tokens = int(len(chunk_text) * 0.7)
    if max_context_length is None:
        if "32B" in model and "AWQ" not in model:
            max_context_length = 32768
        else:
            max_context_length = 40960
    
    base_token_meta = {
        "estimated_input_tokens": estimated_input_tokens,
        "estimated_output_tokens": 0,
        "max_context_length": max_context_length,
        "actual_output_tokens": 0,
        "total_tokens": estimated_input_tokens,
        "used_max_tokens": estimated_input_tokens,
        "prompt_chars": len(chunk_text),
        "text_chars": len(chunk_text),
        "result_chars": 0,
    }
    
    chunk_info = f"å— {chunk_index}" if chunk_index is not None else "å—"
    
    for attempt in range(1, max(1, retries) + 1):
        try:
            out, prompt, token_meta = translate_with_local_llm(
                chunk_text,
                model,
                temperature,
                max_tokens,
                terminology_txt,
                stop=stop,
                frequency_penalty=frequency_penalty,
                presence_penalty=presence_penalty,
                few_shot_samples=few_shot_samples,
                max_context_length=max_context_length,
                preface_file=preface_file,
                bilingual=bilingual,
                stream=stream,
                logger=logger,
            )
            
            # è¿›è¡Œè´¨é‡æ£€æµ‹
            if out and out.strip():
                log_message(logger, f"    å¯¹{chunk_info}è¿›è¡Œè´¨é‡æ£€æµ‹...", "INFO")
                is_good, reason = check_translation_quality_with_llm(
                    chunk_text, out, model, bilingual=bilingual
                )
                
                if is_good:
                    log_message(logger, f"    {chunk_info}è´¨é‡æ£€æµ‹é€šè¿‡: {reason}", "INFO")
                    return out, prompt, True, token_meta
                else:
                    log_message(logger, f"    {chunk_info}è´¨é‡æ£€æµ‹å¤±è´¥: {reason}", "WARNING")
                    if attempt < retries:
                        log_message(logger, f"    è´¨é‡ä¸ä½³ï¼Œé‡è¯•{chunk_info} (å°è¯• {attempt + 1}/{retries})", "WARNING")
                        time.sleep(retry_wait)
                        continue
                    else:
                        log_message(logger, f"    {chunk_info}è´¨é‡ä¸ä½³ä½†å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œè¿”å›ç»“æœ", "WARNING")
                        return out, prompt, True, token_meta
            else:
                log_message(logger, f"    {chunk_info}ç¿»è¯‘ç»“æœä¸ºç©º", "WARNING")
                if attempt < retries:
                    log_message(logger, f"    ç»“æœä¸ºç©ºï¼Œé‡è¯•{chunk_info} (å°è¯• {attempt + 1}/{retries})", "WARNING")
                    time.sleep(retry_wait)
                    continue
                else:
                    log_message(logger, f"    {chunk_info}å¤šæ¬¡é‡è¯•ä»ä¸ºç©ºï¼Œè¿”å›å¤±è´¥", "ERROR")
                    return "", "", False, base_token_meta
            
        except BadRequestError as e:  # ä¾‹å¦‚ä¸Šä¸‹æ–‡æº¢å‡º
            last_err = e
            msg = str(e).lower()
            if any(k in msg for k in ["context", "too many tokens", "maximum context length", "max_tokens must be"]):
                # ä¸Šä¸‹æ–‡ç›¸å…³é”™è¯¯ï¼Œæç¤ºå¤–å±‚é™çº§ä¸ºåˆ†æ®µ
                log_message(logger, f"    {chunk_info}ä¸Šä¸‹æ–‡æº¢å‡ºé”™è¯¯: {e}", "ERROR")
                return "", "", False, base_token_meta
            log_message(logger, f"    {chunk_info}é‡è¯• {attempt}/{retries}: BadRequestError: {e}", "WARNING")
            time.sleep(retry_wait)
            continue
        except Exception as e:
            last_err = e
            log_message(logger, f"    {chunk_info}é‡è¯• {attempt}/{retries}: Exception: {e}", "WARNING")
            time.sleep(retry_wait)
            continue
    
    # å¤šæ¬¡å¤±è´¥ï¼Œè¿”å›åŸºç¡€tokenä¿¡æ¯
    log_message(logger, f"    {chunk_info}æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†ï¼Œæœ€åé”™è¯¯: {last_err}", "ERROR")
    return "", "", False, base_token_meta


def create_bilingual_output(original_text: str, translated_chunks: List[str]) -> str:
    """åˆ›å»ºå¯¹ç…§æ¨¡å¼è¾“å‡ºï¼šé€è¡Œæ—¥è¯­åŸæ–‡ + ä¸­æ–‡è¯‘æ–‡"""
    # è§£æYAML front matter
    article_info = parse_yaml_front_matter(original_text)
    
    # åˆ†ç¦»YAMLéƒ¨åˆ†å’Œæ­£æ–‡éƒ¨åˆ†
    original_lines = original_text.split('\n')
    full_translation = '\n\n'.join(translated_chunks)
    translated_lines = full_translation.split('\n')
    
    # æ‰¾åˆ°YAMLç»“æŸä½ç½®
    yaml_end_idx = -1
    if article_info:
        for i, line in enumerate(original_lines):
            if line.strip() == '---' and i > 0:  # ç¬¬äºŒä¸ª---
                yaml_end_idx = i
                break
    
    bilingual_lines = []
    
    # å¤„ç†YAMLéƒ¨åˆ†ï¼ˆå¦‚æœæœ‰ï¼‰
    if yaml_end_idx > 0:
        # YAMLéƒ¨åˆ†ï¼šåŸæ–‡è¡Œ + è¯‘æ–‡è¡Œäº¤é”™
        for i in range(yaml_end_idx + 1):
            # æ·»åŠ åŸæ–‡è¡Œ
            if i < len(original_lines):
                original_line = original_lines[i]
                bilingual_lines.append(original_line)
            else:
                bilingual_lines.append("")
            
            # æ·»åŠ è¯‘æ–‡è¡Œ
            if i < len(translated_lines):
                translated_line = translated_lines[i]
                bilingual_lines.append(translated_line)
            else:
                bilingual_lines.append("")
    
    # å¤„ç†æ­£æ–‡éƒ¨åˆ†
    if yaml_end_idx >= 0:
        # ä»YAMLç»“æŸåå¼€å§‹å¤„ç†æ­£æ–‡
        original_body_lines = original_lines[yaml_end_idx + 1:]
        # ç¡®ä¿ç¿»è¯‘æ–‡æœ¬ä¹Ÿæœ‰å¯¹åº”çš„æ­£æ–‡éƒ¨åˆ†
        if len(translated_lines) > yaml_end_idx:
            translated_body_lines = translated_lines[yaml_end_idx + 1:]
        else:
            # å¦‚æœç¿»è¯‘æ–‡æœ¬æ²¡æœ‰YAMLéƒ¨åˆ†ï¼Œç›´æ¥ä½¿ç”¨å…¨éƒ¨ç¿»è¯‘æ–‡æœ¬
            translated_body_lines = translated_lines
    else:
        # æ²¡æœ‰YAMLï¼Œç›´æ¥å¤„ç†å…¨éƒ¨
        original_body_lines = original_lines
        translated_body_lines = translated_lines
    
    # æ­£æ–‡éƒ¨åˆ†ï¼šåŸæ–‡è¡Œ + è¯‘æ–‡è¡Œäº¤é”™
    max_body_lines = max(len(original_body_lines), len(translated_body_lines))
    for i in range(max_body_lines):
        # æ·»åŠ åŸæ–‡è¡Œ
        if i < len(original_body_lines):
            original_line = original_body_lines[i]
            bilingual_lines.append(original_line)
        else:
            bilingual_lines.append("")
        
        # æ·»åŠ è¯‘æ–‡è¡Œ
        if i < len(translated_body_lines):
            translated_line = translated_body_lines[i]
            bilingual_lines.append(translated_line)
        else:
            bilingual_lines.append("")
    
    return '\n'.join(bilingual_lines)


def process_file(path: Path, model: str, temperature: float, max_tokens: int, overwrite: bool, log_dir: Path, terminology_file: Optional[Path], chunk_size_chars: int, stop: Optional[List[str]], frequency_penalty: Optional[float], presence_penalty: Optional[float], mode: str, overlap_chars: int, retries: int, retry_wait: float, fallback_on_context: bool, few_shot_samples: Optional[List[Tuple[str, str]]], max_context_length: Optional[int] = None, preface_file: Optional[str] = None, bilingual: bool = False, stream: bool = False, realtime_log: bool = False, no_llm_check: bool = False) -> None:
    # è®¾ç½®æ—¥å¿—
    logger = None
    if realtime_log:
        # æå‰å®šä¹‰log_path
        log_dir.mkdir(parents=True, exist_ok=True)
        ts = time.strftime("%Y%m%d-%H%M%S")
        log_path = log_dir / f"translation_{path.stem}_{ts}.log"
        logger = setup_logging(log_path, stream_output=True)
    else:
        logger = setup_logging(stream_output=True)
    # å…¥å£å¤„è¿›è¡Œæ–‡ä»¶ç±»å‹åˆ¤æ–­ä¸æ¸…ç†ï¼Œé˜²æ­¢è·‘å
    name = path.name
    stem = path.stem
    # 1) è‹¥æ˜¯é‡å¤çš„ _bilingual_bilingual.txtï¼Œç›´æ¥åˆ é™¤åè¿”å›
    if name.endswith("_bilingual_bilingual.txt"):
        try:
            path.unlink()
            log_message(logger, f"DELETE duplicate: {path}")
        except Exception as e:
            log_message(logger, f"WARN åˆ é™¤å¤±è´¥: {path} -> {e}", "WARNING")
        return

    # 2) è‹¥æ˜¯ *_bilingual.txt / *_awq_bilingual.txt -> ä»…è´¨é‡æ£€æµ‹ï¼Œä¸åˆæ ¼åˆ™åˆ é™¤ï¼Œåˆæ ¼åˆ™è·³è¿‡
    if name.endswith("_bilingual.txt") or name.endswith("_awq_bilingual.txt"):
        original_path = None
        if name.endswith("_bilingual.txt"):
            original_path = path.with_name(stem.replace("_bilingual", "") + ".txt")
        elif name.endswith("_awq_bilingual.txt"):
            original_path = path.with_name(stem.replace("_awq_bilingual", "") + ".txt")
        if original_path and original_path.exists():
            try:
                original_text = original_path.read_text(encoding="utf-8", errors="ignore")
                translated_text = path.read_text(encoding="utf-8", errors="ignore")
                if no_llm_check:
                    ok, reason = check_translation_quality_basic(original_text, translated_text, bilingual=True)
                else:
                    ok, reason = check_translation_quality_with_llm(original_text, translated_text, model, bilingual=True)
                if ok:
                    log_message(logger, f"KEEP {path} ({reason})")
                else:
                    log_message(logger, f"DELETE low-quality: {path} ({reason})")
                    try:
                        path.unlink()
                    except Exception as e:
                        log_message(logger, f"WARN åˆ é™¤å¤±è´¥: {path} -> {e}", "WARNING")
                return
            except Exception as e:
                log_message(logger, f"WARN è´¨é‡æ£€æµ‹å¤±è´¥: {path} -> {e}", "WARNING")
                return
        else:
            log_message(logger, f"SKIP no-original: {path}")
            return

    # 3) è‹¥åŒ…å« _zhï¼ˆæˆ– _awq_zhï¼‰ï¼Œå¿½ç•¥
    if name.endswith("_zh.txt") or name.endswith("_awq_zh.txt") or "_zh_" in name:
        log_message(logger, f"SKIP zh file: {path}")
        return

    # 4) åŸæ–‡ï¼šè‹¥å·²å­˜åœ¨ä»»ä¸€ bilingualï¼ˆæˆ– awq_bilingualï¼‰ï¼Œæ£€æŸ¥è´¨é‡åå†³å®šæ˜¯å¦è·³è¿‡
    possible_bi = [
        path.with_name(stem + "_bilingual.txt"),
        path.with_name(stem + "_awq_bilingual.txt"),
    ]
    existing_bilingual_file = None
    for bi in possible_bi:
        if bi.exists():
            existing_bilingual_file = bi
            break
    
    if existing_bilingual_file and not overwrite:
        try:
            original_text = path.read_text(encoding="utf-8", errors="ignore")
            translated_text = existing_bilingual_file.read_text(encoding="utf-8", errors="ignore")
            print(f"    æ£€æŸ¥ç°æœ‰bilingualç¿»è¯‘æ–‡ä»¶è´¨é‡: {existing_bilingual_file.name}")
            if no_llm_check:
                is_good, reason = check_translation_quality_basic(original_text, translated_text, bilingual=True)
            else:
                is_good, reason = check_translation_quality_with_llm(original_text, translated_text, model, bilingual=True)
            if is_good:
                print(f"SKIP {existing_bilingual_file} (ç¿»è¯‘è´¨é‡è‰¯å¥½: {reason})")
                return
            else:
                print(f"DELETE low-quality: {existing_bilingual_file} ({reason})")
                try:
                    existing_bilingual_file.unlink()
                except Exception as e:
                    print(f"WARN åˆ é™¤å¤±è´¥: {existing_bilingual_file} -> {e}")
                # åˆ é™¤åç»§ç»­ç¿»è¯‘
        except Exception as e:
            print(f"WARN è´¨é‡æ£€æµ‹å¤±è´¥: {existing_bilingual_file} -> {e}")
            # å¦‚æœæ£€æŸ¥å¤±è´¥ï¼Œç»§ç»­ç¿»è¯‘
    # æ ¹æ®æ¨¡å‹å’Œæ¨¡å¼åŒºåˆ†è¾“å‡ºæ–‡ä»¶å
    model_upper = (model or "").upper()
    if bilingual:
        zh_suffix = "_awq_bilingual.txt" if "AWQ" in model_upper else "_bilingual.txt"
    else:
        zh_suffix = "_awq_zh.txt" if "AWQ" in model_upper else "_zh.txt"
    zh_path = path.with_name(path.stem + zh_suffix)
    
    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç¿»è¯‘æ–‡ä»¶ä¸”è´¨é‡è‰¯å¥½ï¼ˆébilingualæ¨¡å¼ï¼‰
    if not bilingual and zh_path.exists() and not overwrite:
        try:
            existing_translation = zh_path.read_text(encoding="utf-8", errors="ignore")
            print(f"    æ£€æŸ¥ç°æœ‰ç¿»è¯‘æ–‡ä»¶è´¨é‡...")
            # å…ˆè¯»å–åŸæ–‡
            raw_text = path.read_text(encoding="utf-8", errors="ignore")
            if no_llm_check:
                is_good, reason = check_translation_quality_basic(raw_text, existing_translation)
            else:
                is_good, reason = check_translation_quality_with_llm(raw_text, existing_translation, model)
            if is_good:
                print(f"SKIP {zh_path} (ç¿»è¯‘è´¨é‡è‰¯å¥½: {reason})")
                return
            else:
                print(f"REWRITE {zh_path} (ç¿»è¯‘è´¨é‡ä¸ä½³: {reason})")
        except Exception as e:
            print(f"WARN æ£€æŸ¥ç°æœ‰ç¿»è¯‘æ–‡ä»¶å¤±è´¥: {e}")
            # å¦‚æœæ£€æŸ¥å¤±è´¥ï¼Œç»§ç»­ç¿»è¯‘
    
    # è¯»å–åŸæ–‡
    raw_text = path.read_text(encoding="utf-8", errors="ignore")
    if not raw_text.strip():
        log_message(logger, f"WARN empty file: {path}", "WARNING")
        return
    
    # è§£æYAML front matterè·å–æ–‡ç« ä¿¡æ¯
    article_info = parse_yaml_front_matter(raw_text)
    if article_info:
        log_message(logger, f"ğŸ“– æ–‡ç« ä¿¡æ¯:")
        log_message(logger, f"   æ ‡é¢˜: {article_info.get('title', 'æœªçŸ¥')}")
        log_message(logger, f"   ä½œè€…: {article_info.get('author', {}).get('name', 'æœªçŸ¥')}")
        log_message(logger, f"   ç³»åˆ—: {article_info.get('series', {}).get('title', 'æ— ç³»åˆ—')}")
        log_message(logger, f"   åˆ›å»ºæ—¶é—´: {article_info.get('create_date', 'æœªçŸ¥')}")
        log_message(logger, f"   åŸæ–‡é•¿åº¦: {len(raw_text)} å­—ç¬¦")
        log_message(logger, f"   æ ‡ç­¾: {', '.join(article_info.get('tags', []))}")
    else:
        log_message(logger, f"ğŸ“– æ–‡ç« ä¿¡æ¯: æ— æ³•è§£æYAML front matter")
        log_message(logger, f"   åŸæ–‡é•¿åº¦: {len(raw_text)} å­—ç¬¦")
    
    log_message(logger, f"ğŸ”§ ç¿»è¯‘é…ç½®:")
    log_message(logger, f"   æ¨¡å‹: {model}")
    log_message(logger, f"   æ¨¡å¼: {mode}")
    log_message(logger, f"   å¯¹ç…§æ¨¡å¼: {bilingual}")
    log_message(logger, f"   æµå¼è¾“å‡º: {stream}")
    log_message(logger, f"   å®æ—¶æ—¥å¿—: {realtime_log}")
    log_message(logger, f"   å—å¤§å°: {chunk_size_chars} å­—ç¬¦")
    log_message(logger, f"   é‡å å¤§å°: {overlap_chars} å­—ç¬¦")
    log_message(logger, f"   é‡è¯•æ¬¡æ•°: {retries}")
    log_message(logger, f"   é‡è¯•ç­‰å¾…: {retry_wait} ç§’")
    log_message(logger, f"   ä¸Šä¸‹æ–‡é•¿åº¦: {max_context_length or 'é»˜è®¤'}")
    log_message(logger, f"   æ¸©åº¦: {temperature}")
    log_message(logger, f"   é¢‘ç‡æƒ©ç½š: {frequency_penalty}")
    log_message(logger, f"   å­˜åœ¨æƒ©ç½š: {presence_penalty}")
    log_message(logger, f"   æœ¯è¯­æ–‡ä»¶: {terminology_file or 'æ— '}")
    log_message(logger, f"   ç¤ºä¾‹æ–‡ä»¶: {len(few_shot_samples) if few_shot_samples else 0} ä¸ªç¤ºä¾‹")
    log_message(logger, f"   å‰è¨€æ–‡ä»¶: {preface_file or 'æ— '}")
    log_message(logger, f"   åœæ­¢è¯: {stop or 'æ— '}")
    log_message(logger, f"   æ—¥å¿—ç›®å½•: {log_dir}")
    log_message(logger, f"   è¾“å‡ºæ–‡ä»¶: {zh_path}")
    log_message(logger, f"   {'='*50}")
    
    # ç»„è£…å…¨æ–‡æˆ–åˆ†å—ï¼šé»˜è®¤å¯¹"æ•´ä¸ªåŸæ–‡ä»¶æ–‡æœ¬ï¼ˆå«YAMLä¸æ­£æ–‡ï¼‰"è¿›è¡Œä¸€æ¬¡æ€§ç¿»è¯‘
    chunks: List[str] = []
    if mode == "full":
        chunks = [raw_text]
    else:
        # ä½¿ç”¨æŒ‰è¡Œåˆ†å‰²ï¼Œç¡®ä¿æ•´è¡Œä¸è¢«æˆªæ–­
        chunks = split_text_by_lines(raw_text, chunk_size_chars, overlap_chars)
    terminology_txt: Optional[str] = None
    if terminology_file and terminology_file.exists():
        try:
            terminology_txt = terminology_file.read_text(encoding="utf-8")
        except Exception:
            terminology_txt = None
    start = time.time()
    outputs: List[str] = []
    prompts: List[str] = []
    # ç›´æ¥å¯¹æ•´ä»½è¾“å…¥æ–‡æœ¬è¿›è¡Œç¿»è¯‘ï¼ˆåŒ…å« YAML ä¸æ­£æ–‡ï¼‰ï¼Œä¸å†åˆ†å¼€å¤„ç†æ ‡é¢˜/å‰è¨€

    def chunkify_with_overlap_raw(text: str) -> List[str]:
        out_chunks: List[str] = []
        step_local = max(1, chunk_size_chars - max(0, overlap_chars))
        i = 0
        L = len(text)
        while i < L:
            j = min(L, i + chunk_size_chars)
            c = text[max(0, i - max(0, overlap_chars)): j]
            out_chunks.append(c)
            if j >= L:
                break
            i += step_local
        return out_chunks

    # è®¾ç½®å®æ—¶æ—¥å¿—è¾“å‡º
    logger = None
    if realtime_log:
        # æå‰å®šä¹‰log_path
        log_dir.mkdir(parents=True, exist_ok=True)
        ts = time.strftime("%Y%m%d-%H%M%S")
        log_path = log_dir / f"translation_{path.stem}_{ts}.log"
        
        logger = setup_realtime_logging(log_path, stream_output=True)
        log_realtime(logger, f"å¼€å§‹å¤„ç†æ–‡ä»¶: {path}")
        log_realtime(logger, f"æ¨¡å‹: {model}, æ¨¡å¼: {mode}, å¯¹ç…§æ¨¡å¼: {bilingual}")

    def run_chunks(active_chunks: List[str]) -> Tuple[List[str], List[str], bool, List[Dict[str, int]]]:
        outs: List[str] = []
        prms: List[str] = []
        metas: List[Dict[str, int]] = []
        log_message(logger, f"å¼€å§‹å¤„ç† {len(active_chunks)} ä¸ªå—...")
        for i, ck in enumerate(active_chunks, 1):
            log_message(logger, f"å¤„ç†ç¬¬ {i}/{len(active_chunks)} å—ï¼Œé•¿åº¦: {len(ck)}")
            out, prompt, ok, token_meta = translate_chunk_with_retry(
                ck, model, temperature, max_tokens, terminology_txt, stop, frequency_penalty, presence_penalty, retries, retry_wait, few_shot_samples, max_context_length, preface_file, bilingual, stream, logger, chunk_index=i
            )
            log_message(logger, f"ç¬¬ {i} å—ç»“æœ: ok={ok}, out_len={len(out)}, prompt_len={len(prompt)}")
            if not ok:
                log_message(logger, f"ç¬¬ {i} å—ç¿»è¯‘å¤±è´¥", "WARNING")
                # å³ä½¿å¤±è´¥ï¼Œä¹Ÿä¿å­˜ç»“æœç”¨äºè°ƒè¯•
                if out.strip():
                    cleaned_out = clean_output_text(out)
                    if not cleaned_out.strip():
                        cleaned_out = out
                    outs.append(cleaned_out)
                    prms.append(prompt)
                metas.append(token_meta)
                return outs, prms, False, metas
            # ç®€å•é‡å¤æ£€æµ‹ï¼šè‹¥æŸå—è¾“å‡ºä¸ä¸Šä¸€å—é«˜åº¦ç›¸ä¼¼ï¼ˆå‰200å­—ç¬¦ç›¸åŒï¼‰ï¼Œåˆ™æˆªæ–­
            if outs and out[:200] == outs[-1][:200]:
                out = out[: max(200, len(out)//2)]
            # æ¸…ç†è¾“å‡ºæ–‡æœ¬ï¼Œå»é™¤æ€è€ƒéƒ¨åˆ†
            log_message(logger, f"åŸå§‹è¾“å‡ºé•¿åº¦: {len(out)}")
            cleaned_out = clean_output_text(out)
            log_message(logger, f"æ¸…ç†åé•¿åº¦: {len(cleaned_out)}")
            if not cleaned_out.strip():
                log_message(logger, f"è­¦å‘Š: æ¸…ç†åè¾“å‡ºä¸ºç©ºï¼Œä½¿ç”¨åŸå§‹è¾“å‡º", "WARNING")
                cleaned_out = out
            outs.append(cleaned_out)
            prms.append(prompt)
            metas.append(token_meta)
        return outs, prms, True, metas

    # é¦–æ¬¡å°è¯•ï¼šæŒ‰å½“å‰æ¨¡å¼æ‰§è¡Œ
    outs, prms, ok, metas = run_chunks(chunks)
    
    # å¦‚æœé¦–æ¬¡å°è¯•å¤±è´¥ï¼Œå°è¯•åˆ†å—é‡è¯•
    if (not ok) and fallback_on_context and mode == "full":
        log_message(logger, f"é¦–æ¬¡ç¿»è¯‘å¤±è´¥ï¼Œå°è¯•åˆ†å—é‡è¯•...")
        # é™çº§ä¸ºå¯¹"åŸå§‹å…¨æ–‡"çš„å­—ç¬¦é‡å åˆ‡åˆ†
        chunks2 = split_text_by_lines(raw_text, chunk_size_chars, overlap_chars)
        outs, prms, ok, metas = run_chunks(chunks2)
    
    # å³ä½¿æ£€æµ‹åˆ°åè¾“å‡ºï¼Œä¹Ÿä¿å­˜ç»“æœ
    if outs:
        outputs.extend(outs)
        prompts.extend(prms)
        log_message(logger, f"ä¿å­˜äº† {len(outs)} ä¸ªè¾“å‡ºå—")
    else:
        log_message(logger, f"è­¦å‘Š: æ²¡æœ‰æœ‰æ•ˆçš„è¾“å‡ºå—", "WARNING")
    
    # æ ¹æ®æ¨¡å¼å¤„ç†è¾“å‡º
    if bilingual:
        # å¯¹ç…§æ¨¡å¼ï¼šç›´æ¥ä½¿ç”¨æ¨¡å‹è¾“å‡ºçš„å¯¹ç…§æ ¼å¼
        translation = "\n\n".join(outputs)
    else:
        # å•è¯­æ¨¡å¼ï¼šåŸæœ‰é€»è¾‘
        translation = "\n\n".join(outputs)
    
    # è´¨é‡æ£€æµ‹ï¼šå¦‚æœç¿»è¯‘ä¸å®Œæ•´ï¼Œå°è¯•åˆ†å—é‡è¯•
    if fallback_on_context and mode == "full":
        log_message(logger, f"è¿›è¡Œç¿»è¯‘è´¨é‡æ£€æµ‹...")
        if no_llm_check:
            is_good, reason = check_translation_quality_basic(raw_text, translation, bilingual)
        else:
            is_good, reason = check_translation_quality_with_llm(raw_text, translation, model, bilingual)
        
        if not is_good:
            log_message(logger, f"ç¿»è¯‘è´¨é‡æ£€æµ‹å¤±è´¥: {reason}")
            log_message(logger, f"å°è¯•åˆ†å—é‡è¯•...")
            # ä½¿ç”¨æ›´å°çš„å—å¤§å°è¿›è¡Œåˆ†å—é‡è¯•
            chunk_size_for_fallback = min(8000, chunk_size_chars // 2)  # ä½¿ç”¨æ›´å°çš„å—å¤§å°
            chunks2 = split_text_by_lines(raw_text, chunk_size_for_fallback, overlap_chars)
            log_message(logger, f"åˆ†å—é‡è¯•: ä½¿ç”¨ {len(chunks2)} ä¸ªå—ï¼Œæ¯å—æœ€å¤§ {chunk_size_for_fallback} å­—ç¬¦")
            outs2, prms2, ok2, metas2 = run_chunks(chunks2)
            if outs2:
                outputs = outs2
                prompts = prms2
                if bilingual:
                    translation = "\n\n".join(outputs)
                else:
                    translation = "\n\n".join(outputs)
                log_message(logger, f"åˆ†å—é‡è¯•å®Œæˆï¼Œä¿å­˜äº† {len(outs2)} ä¸ªè¾“å‡ºå—")
            else:
                log_message(logger, f"åˆ†å—é‡è¯•ä¹Ÿå¤±è´¥äº†", "WARNING")
        else:
            log_message(logger, f"ç¿»è¯‘è´¨é‡æ£€æµ‹é€šè¿‡: {reason}")
    full_prompt = "\n\n".join(prompts)
    cost = time.time() - start

    zh_path.write_text(translation, encoding="utf-8")
    
    # æœ€ç»ˆè¾“å‡º
    log_message(logger, f"WRITE {zh_path} ({cost:.1f}s)")


def expand_inputs(inputs: List[str]) -> List[Path]:
    files: List[Path] = []
    for p in inputs:
        pth = Path(p)
        if pth.is_dir():
            files.extend(sorted(pth.glob("*.txt")))
        else:
            # æ”¯æŒé€šé…ç¬¦
            for m in glob.glob(p):
                mp = Path(m)
                if mp.is_file() and mp.suffix == ".txt":
                    files.append(mp)
    # ä»…ä¿ç•™æºæ–‡ä»¶ï¼ˆä¸å« _zh.txtï¼‰
    files = [f for f in files if not f.name.endswith("_zh.txt")]
    return files


def main() -> None:
    parser = argparse.ArgumentParser(description="æ‰¹é‡ç¿»è¯‘ Pixiv å°è¯´åˆ° _zh.txt")
    parser.add_argument("inputs", nargs="+", help="è¾“å…¥ï¼šæ–‡ä»¶/ç›®å½•/é€šé…ç¬¦")
    parser.add_argument("--model", default="Qwen/Qwen3-32B-AWQ")
    parser.add_argument("--temperature", type=float, default=0.1)
    parser.add_argument("--max-tokens", type=int, default=0, help="<=0 è¡¨ç¤ºä¸é™åˆ¶ï¼ˆä¸ä¼ è¯¥å‚æ•°ï¼‰")
    parser.add_argument("--max-context-length", type=int, default=None, help="æ¨¡å‹çš„æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œå¦‚æœä¸æŒ‡å®šåˆ™æ ¹æ®æ¨¡å‹åç§°è‡ªåŠ¨æ¨æ–­")
    parser.add_argument("--overwrite", action="store_true")
    parser.add_argument("--log-dir", default="logs")
    parser.add_argument("--terminology-file", type=Path, default=Path("tasks/translation/data/terminology.txt"))
    parser.add_argument("--chunk-size-chars", type=int, default=20000, help="åˆ†å—æ¨¡å¼ä¸‹æ¯æ¬¡è¯·æ±‚çš„æœ€å¤§å­—ç¬¦æ•°")
    parser.add_argument("--stop", nargs="*", default=["ï¼ˆæœªå®Œå¾…ç»­ï¼‰", "[END]"], help="ç”Ÿæˆåœæ­¢è¯")
    parser.add_argument("--frequency-penalty", type=float, default=0.3)
    parser.add_argument("--presence-penalty", type=float, default=0.2, help="presence penalty å‚æ•°")
    parser.add_argument("--mode", choices=["full", "chunked"], default="full", help="å…¨æ–‡ä¸€æ¬¡æ€§æˆ–åˆ†å—æ¨¡å¼")
    parser.add_argument("--overlap-chars", type=int, default=1000, help="åˆ†å—æ¨¡å¼ä¸‹ç›¸é‚»å—é‡å å­—ç¬¦æ•°")
    parser.add_argument("--retries", type=int, default=3, help="æ¯å—æœ€å¤§é‡è¯•æ¬¡æ•°")
    parser.add_argument("--retry-wait", type=float, default=2.0, help="é‡è¯•å‰ç­‰å¾…ç§’æ•°")
    parser.add_argument("--fallback-on-context", action="store_true", help="ä¸Šä¸‹æ–‡æº¢å‡ºæ—¶è‡ªåŠ¨é™çº§ä¸ºåˆ†å—")
    parser.add_argument("--limit", type=int, default=0, help="é™åˆ¶å¤„ç†çš„æ–‡ä»¶æ•°é‡ï¼Œ0è¡¨ç¤ºä¸é™åˆ¶")
    parser.add_argument("--sample-file", type=Path, default=Path("tasks/translation/data/samples/sample.txt"), help="few-shot ç¤ºä¾‹æ–‡ä»¶")
    parser.add_argument("--preface-file", type=Path, default=Path("tasks/translation/data/preface.txt"), help="ç¿»è¯‘æŒ‡ä»¤æ¨¡æ¿æ–‡ä»¶")
    parser.add_argument("--bilingual", action="store_true", help="å¯ç”¨å¯¹ç…§æ¨¡å¼ï¼šè¾“å‡ºæ—¥è¯­åŸæ–‡+ä¸­æ–‡è¯‘æ–‡äº¤é”™æ ¼å¼")
    parser.add_argument("--stream", action="store_true", help="å¯ç”¨æµå¼è¾“å‡ºï¼Œå®æ—¶æ˜¾ç¤ºæ¨¡å‹ç”Ÿæˆè¿‡ç¨‹")
    parser.add_argument("--realtime-log", action="store_true", help="å¯ç”¨å®æ—¶æ—¥å¿—è¾“å‡ºï¼ŒåŒæ—¶æ˜¾ç¤ºåœ¨æ§åˆ¶å°å’Œæ–‡ä»¶ä¸­")
    parser.add_argument("--no-llm-check", action="store_true", help="ç¦ç”¨å¤§æ¨¡å‹è´¨é‡æ£€æµ‹ï¼Œä½¿ç”¨åŸºç¡€æ£€æµ‹")
    parser.add_argument("--strict-repetition-check", action="store_true", help="å¯ç”¨ä¸¥æ ¼é‡å¤æ£€æµ‹ï¼Œæ›´æ—©å‘ç°å¹¶æˆªæ–­é‡å¤è¾“å‡º")
    args = parser.parse_args()

    files = expand_inputs(args.inputs)
    if not files:
        print("no files matched")
        sys.exit(1)

    # é™åˆ¶å¤„ç†æ–‡ä»¶æ•°é‡
    if args.limit > 0:
        files = files[:args.limit]
        print(f"é™åˆ¶å¤„ç†å‰ {args.limit} ä¸ªæ–‡ä»¶")

    # åŠ è½½ few-shot ç¤ºä¾‹
    few_shot_samples = load_few_shot_samples(args.sample_file)
    if few_shot_samples:
        print(f"åŠ è½½äº† {len(few_shot_samples)} ä¸ª few-shot ç¤ºä¾‹")
        # è°ƒè¯•è¾“å‡ºï¼šæ‰“å°å®Œæ•´çš„few-shotç¤ºä¾‹
        print("=" * 50)
        print("å®Œæ•´çš„ few-shot ç¤ºä¾‹:")
        print("=" * 50)
        for i, (input_text, output_text) in enumerate(few_shot_samples, 1):
            print(f"ç¤ºä¾‹ {i}:")
            print("è¾“å…¥:")
            print(input_text)
            print("è¾“å‡º:")
            print(output_text)
            print("-" * 30)

    for f in files:
        process_file(
            f,
            model=args.model,
            temperature=args.temperature,
            max_tokens=args.max_tokens,
            overwrite=args.overwrite,
            log_dir=Path(args.log_dir),
            terminology_file=args.terminology_file,
            chunk_size_chars=args.chunk_size_chars,
            stop=args.stop,
            frequency_penalty=args.frequency_penalty,
            presence_penalty=args.presence_penalty,
            mode=args.mode,
            overlap_chars=args.overlap_chars,
            retries=args.retries,
            retry_wait=args.retry_wait,
            fallback_on_context=args.fallback_on_context,
            few_shot_samples=few_shot_samples,
            max_context_length=args.max_context_length,
            preface_file=str(args.preface_file),
            bilingual=args.bilingual,
            stream=args.stream,
            realtime_log=args.realtime_log,
            no_llm_check=args.no_llm_check,
        )


if __name__ == "__main__":
    main()


