#!/usr/bin/env python3
"""
Tokenè®¡æ•°å·¥å…·
æ”¯æŒé€šè¿‡å‘½ä»¤è¡Œå‚æ•°æŒ‡å®šæ–‡ä»¶æˆ–ç›®å½•
"""

import os
import sys
import argparse
from transformers import AutoTokenizer

def count_tokens_in_file(file_path, tokenizer):
    """è®¡ç®—å•ä¸ªæ–‡ä»¶çš„tokenæ•°é‡"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            text = f.read()
        
        tokens = tokenizer.encode(text)
        return len(tokens)
    except Exception as e:
        print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
        return 0

def count_tokens_in_directory(dir_path, tokenizer):
    """è®¡ç®—ç›®å½•ä¸­æ‰€æœ‰txtæ–‡ä»¶çš„tokenæ•°é‡"""
    total_tokens = 0
    file_count = 0
    
    for root, dirs, files in os.walk(dir_path):
        for file in files:
            if file.endswith('.txt'):
                file_path = os.path.join(root, file)
                tokens = count_tokens_in_file(file_path, tokenizer)
                if tokens > 0:
                    print(f"ğŸ“„ {file_path}: {tokens:,} tokens")
                    total_tokens += tokens
                    file_count += 1
    
    return total_tokens, file_count

def main():
    parser = argparse.ArgumentParser(description="Tokenè®¡æ•°å·¥å…·")
    parser.add_argument("path", help="æ–‡ä»¶æˆ–ç›®å½•è·¯å¾„")
    parser.add_argument("--model", "-m", default="Qwen/Qwen3-32B", help="ä½¿ç”¨çš„tokenizeræ¨¡å‹")
    
    args = parser.parse_args()
    
    if not os.path.exists(args.path):
        print(f"âŒ è·¯å¾„ä¸å­˜åœ¨: {args.path}")
        sys.exit(1)
    
    print(f"ğŸ”§ åŠ è½½tokenizer: {args.model}")
    try:
        tokenizer = AutoTokenizer.from_pretrained(args.model, trust_remote_code=True)
    except Exception as e:
        print(f"âŒ åŠ è½½tokenizerå¤±è´¥: {e}")
        sys.exit(1)
    
    if os.path.isfile(args.path):
        # å•ä¸ªæ–‡ä»¶
        tokens = count_tokens_in_file(args.path, tokenizer)
        print(f"ğŸ“Š æ–‡ä»¶: {args.path}")
        print(f"ğŸ“Š Tokenæ•°é‡: {tokens:,}")
    else:
        # ç›®å½•
        print(f"ğŸ“ æ‰«æç›®å½•: {args.path}")
        total_tokens, file_count = count_tokens_in_directory(args.path, tokenizer)
        print(f"ğŸ“Š æ€»æ–‡ä»¶æ•°: {file_count}")
        print(f"ğŸ“Š æ€»Tokenæ•°: {total_tokens:,}")

if __name__ == "__main__":
    main()
