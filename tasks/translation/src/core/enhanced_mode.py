#!/usr/bin/env python3
"""
å¢å¼ºæ¨¡å¼å¤„ç†æ¨¡å—
å®ç°QCæ£€æµ‹ + é‡æ–°ç¿»è¯‘åŠŸèƒ½
"""

import re
from pathlib import Path
from typing import List, Tuple, Dict, Optional
from dataclasses import dataclass

from .config import TranslationConfig
from .streaming_handler import StreamingHandler
from .logger import UnifiedLogger
from .prompt import PromptBuilder, create_config


@dataclass
class QCResult:
    """QCæ£€æµ‹ç»“æœ"""
    line_index: int
    original_text: str
    translated_text: str
    quality_score: float
    needs_retranslation: bool
    reason: str = ""


class EnhancedModeHandler:
    """å¢å¼ºæ¨¡å¼å¤„ç†å™¨"""
    
    def __init__(self, config: TranslationConfig, logger: UnifiedLogger):
        self.config = config
        self.logger = logger
        
        # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
        from openai import OpenAI
        self.client = OpenAI(base_url="http://localhost:8000/v1", api_key="dummy")
        
        self.streaming_handler = StreamingHandler(self.client, logger, config)
        self.previous_improvements = {}  # è·Ÿè¸ªä¹‹å‰çš„æ”¹è¿›
        
        # åˆå§‹åŒ–PromptBuilder
        prompt_data_dir = Path(__file__).parent.parent.parent / "data" / "prompt"
        qc_config = create_config("qc", prompt_data_dir)
        # æ‰‹åŠ¨è®¾ç½®æ­£ç¡®çš„æ–‡ä»¶è·¯å¾„
        qc_config.preface_file = "preface_qc.txt"
        qc_config.sample_file = "sample_qc.txt"
        self.qc_prompt_builder = PromptBuilder(qc_config)
        
        enhancement_config = create_config("enhancement", prompt_data_dir)
        # æ‰‹åŠ¨è®¾ç½®æ­£ç¡®çš„æ–‡ä»¶è·¯å¾„
        enhancement_config.preface_file = "preface_enhanced.txt"
        enhancement_config.sample_file = "sample_enhanced.txt"
        enhancement_config.terminology_file = "terminology.txt"
        self.enhancement_prompt_builder = PromptBuilder(enhancement_config)
    
    def process_bilingual_file(self, file_path: Path) -> bool:
        """
        å¤„ç†åŒè¯­æ–‡ä»¶ï¼Œè¿›è¡ŒQCæ£€æµ‹å’Œé‡æ–°ç¿»è¯‘
        
        Args:
            file_path: åŒè¯­æ–‡ä»¶è·¯å¾„
            
        Returns:
            bool: å¤„ç†æ˜¯å¦æˆåŠŸ
        """
        try:
            self.logger.info(f"å¼€å§‹å¢å¼ºæ¨¡å¼å¤„ç†: {file_path}")
            # è®°å½•å½“å‰å¤„ç†æ–‡ä»¶ï¼Œä¾¿äºæ‰¹æ¬¡æ‰“å°
            self.current_processing_file = file_path
            # è¿›å…¥æ—¶å³ç¡®å®šè¾“å‡ºè·¯å¾„å¹¶åœ¨ copy ç­–ç•¥ä¸‹é¢„åˆ›å»ºç›®æ ‡æ–‡ä»¶ï¼Œæ‰“å°è·¯å¾„ï¼ˆå¯¹é½ bilingual_simple è¡Œä¸ºï¼‰
            target_path = self._resolve_output_path(file_path)
            # è‹¥æœªæŒ‡å®šè¦†ç›–ä¸”è¾“å‡ºå·²å­˜åœ¨ï¼Œåˆ™ç›´æ¥è·³è¿‡ï¼ˆä¸æ™®é€šæµç¨‹ä¸€è‡´ï¼‰
            try:
                if not getattr(self.config, 'overwrite', False) and target_path.exists():
                    self.logger.info(f"è¾“å‡ºæ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡: {target_path}")
                    return True
            except Exception:
                # å®¹é”™ï¼šç›®æ ‡è·¯å¾„æ£€æŸ¥å¼‚å¸¸æ—¶ç»§ç»­åç»­æµç¨‹
                pass
            try:
                # è¯»å–åŸæ–‡è¡Œï¼ˆç”¨äºå¯èƒ½çš„é¢„åˆ›å»ºï¼‰
                lines_peek = self._read_bilingual_file(file_path)
                if self.config.enhanced_output == 'copy':
                    # é¢„åˆ›å»ºï¼ˆå¤åˆ¶åŸæ–‡ä»¶ï¼Œä¸åšå ä½å¡«å……ï¼‰
                    target_path.parent.mkdir(parents=True, exist_ok=True)
                    target_path.write_text(''.join(lines_peek), encoding='utf-8')
                # æ‰“å°è¾“å‡ºä¸æ—¥å¿—ä½ç½®ï¼ˆdebug ä¸‹ä¸ç›®æ ‡åŒç›®å½•ï¼›é debug æ—¥å¿—åœ¨ logs/ï¼‰
                log_file = self.logger.get_log_file_path() if hasattr(self.logger, 'get_log_file_path') else None
                self.logger.info(f"ğŸ“„ è¾“å‡ºæ–‡ä»¶: {target_path}", mode=UnifiedLogger.LogMode.BOTH)
                if log_file:
                    self.logger.info(f"ğŸ“ æ—¥å¿—æ–‡ä»¶è·¯å¾„: {log_file}", mode=UnifiedLogger.LogMode.BOTH)
            except Exception as e:
                self.logger.warning(f"é¢„åˆ›å»º/æ‰“å°å¢å¼ºè¾“å‡ºæ–‡ä»¶è·¯å¾„å¤±è´¥: {e}")
            
            # è¯»å–åŒè¯­æ–‡ä»¶
            lines = self._read_bilingual_file(file_path)
            if not lines:
                self.logger.error(f"æ— æ³•è¯»å–æ–‡ä»¶: {file_path}")
                return False
            
            # è§£æåŒè¯­å†…å®¹
            content_lines = self._parse_bilingual_content(lines)
            if not content_lines:
                self.logger.error(f"æ— æ³•è§£æåŒè¯­å†…å®¹: {file_path}")
                return False
            
            # ç›´æ¥ä½¿ç”¨å¢å¼ºæ¨¡å‹å¤„ç†æ‰€æœ‰è¡Œï¼ˆè·³è¿‡QCæ£€æµ‹ï¼‰
            self.logger.info(f"å¼€å§‹å¢å¼ºæ¨¡å¼å¤„ç†: æ€»è¡Œæ•°={len(content_lines)}")
            retranslated_lines = self._enhance_all_lines_batch(content_lines, original_lines=lines, target_path=target_path)

            # è®¡ç®—ç›®æ ‡è¾“å‡ºè·¯å¾„ï¼ˆé»˜è®¤å¤åˆ¶è¾“å‡ºï¼‰
            target_path = self._resolve_output_path(file_path)
            if target_path != file_path:
                # ç›®æ ‡æ–‡ä»¶åº”åœ¨è¿›å…¥æ—¶å·²å¤åˆ¶ï¼›æ­¤å¤„åªåšè¡Œçº§æ›´æ–°
                self._update_bilingual_file(target_path, lines, content_lines, retranslated_lines)
            else:
                # åŸåœ°æ”¹å†™
                self._update_bilingual_file(file_path, lines, content_lines, retranslated_lines)

            # æ‰“å°è¾“å‡ºä¸æ—¥å¿—ä½ç½®
            log_file = self.logger.get_log_file_path() if hasattr(self.logger, 'get_log_file_path') else None
            self.logger.info(f"ğŸ“„ è¾“å‡ºæ–‡ä»¶: {target_path}", mode=UnifiedLogger.LogMode.BOTH)
            if log_file:
                self.logger.info(f"ğŸ“ æ—¥å¿—æ–‡ä»¶è·¯å¾„: {log_file}", mode=UnifiedLogger.LogMode.BOTH)
            
            self.logger.info(f"å¢å¼ºæ¨¡å¼å¤„ç†å®Œæˆ: {file_path}")
            return True
            
        except Exception as e:
            self.logger.error(f"å¢å¼ºæ¨¡å¼å¤„ç†å¤±è´¥: {file_path}, é”™è¯¯: {e}")
            return False
    
    def _read_bilingual_file(self, file_path: Path) -> List[str]:
        """è¯»å–åŒè¯­æ–‡ä»¶"""
        with open(file_path, 'r', encoding='utf-8') as f:
            return f.readlines()
    
    def _parse_bilingual_content(self, lines: List[str]) -> List[Tuple[str, str]]:
        """
        è§£æåŒè¯­å†…å®¹ï¼Œæå–åŸæ–‡å’Œè¯‘æ–‡å¯¹
        æ ¼å¼ï¼šåŸæ–‡è¡Œ\nè¯‘æ–‡è¡Œ\nï¼ˆbilingual_simpleæ¨¡å¼ï¼‰
        
        Returns:
            List[Tuple[str, str]]: [(åŸæ–‡, è¯‘æ–‡), ...]
        """
        content_lines = []
        i = 0
        
        # æ‰¾åˆ°YAMLç»“æŸä½ç½®ï¼šè·³è¿‡æ‰€æœ‰YAMLå†…å®¹ç›´åˆ°ç¬¬äºŒä¸ª---æˆ–å®é™…å†…å®¹å¼€å§‹
        yaml_started = False
        while i < len(lines):
            line = lines[i].strip()
            if line.startswith('---'):
                if not yaml_started:
                    yaml_started = True
                else:
                    # æ‰¾åˆ°ç¬¬äºŒä¸ª---ï¼ŒYAMLç»“æŸ
                    i += 1
                    break
            elif yaml_started and not line.startswith('---'):
                # åœ¨YAMLä¸­ï¼Œç»§ç»­è·³è¿‡
                pass
            elif not yaml_started and line and not line.startswith('---'):
                # æ²¡æœ‰YAMLï¼Œç›´æ¥å¼€å§‹è§£æå†…å®¹
                break
            i += 1
        
        # è§£æåŒè¯­å†…å®¹ï¼šæŒ‰ç…§bilingual_simpleæ ¼å¼ï¼ˆåŸæ–‡è¡Œ\nè¯‘æ–‡è¡Œ\nï¼‰
        while i < len(lines):
            line = lines[i].strip()
            if not line:
                i += 1
                continue
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯åŸæ–‡è¡Œï¼ˆä¸åŒ…å«[ç¿»è¯‘æœªå®Œæˆ]æ ‡è®°ï¼‰
            if not line.startswith('[') and not line.endswith(']'):
                original = line
                i += 1
                
                # æŸ¥æ‰¾å¯¹åº”çš„è¯‘æ–‡è¡Œ
                if i < len(lines):
                    translated = lines[i].strip()
                    # æ£€æŸ¥æ˜¯å¦æ˜¯è¯‘æ–‡è¡Œï¼ˆåŒ…æ‹¬[ç¿»è¯‘æœªå®Œæˆ]æ ‡è®°ï¼‰
                    if translated == "[ç¿»è¯‘æœªå®Œæˆ]":
                        # æ‰¾åˆ°[ç¿»è¯‘æœªå®Œæˆ]æ ‡è®°ï¼Œå°†å…¶ä½œä¸ºè¯‘æ–‡å¤„ç†
                        content_lines.append((original, "[ç¿»è¯‘æœªå®Œæˆ]"))
                        self.logger.debug(f"æœªç¿»è¯‘: åŸæ–‡='{original}', è¯‘æ–‡='[ç¿»è¯‘æœªå®Œæˆ]'")
                    else:
                        # æ­£å¸¸è¯‘æ–‡
                        content_lines.append((original, translated))
                        self.logger.debug(f"è§£ææˆåŠŸ: åŸæ–‡='{original}', è¯‘æ–‡='{translated}'")
                    i += 1
                else:
                    content_lines.append((original, "[ç¿»è¯‘æœªå®Œæˆ]"))
                    self.logger.debug(f"æ–‡ä»¶ç»“æŸ: åŸæ–‡='{original}', è¯‘æ–‡='[ç¿»è¯‘æœªå®Œæˆ]'")
            else:
                # è·³è¿‡å…¶ä»–è¡Œï¼ˆå¦‚[ç¿»è¯‘æœªå®Œæˆ]æ ‡è®°ç­‰ï¼‰
                self.logger.debug(f"è·³è¿‡æ ‡è®°è¡Œ: '{line}'")
                i += 1
        
        self.logger.info(f"è§£æå®Œæˆ: å…±{len(content_lines)}å¯¹åŒè¯­å†…å®¹")
        return content_lines
    
    def _contains_chinese(self, text: str) -> bool:
        """æ£€æŸ¥æ–‡æœ¬æ˜¯å¦åŒ…å«ä¸­æ–‡å­—ç¬¦ï¼ˆæ’é™¤æ—¥æ–‡ï¼‰"""
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å¹³å‡åã€ç‰‡å‡åæˆ–æ—¥æ–‡ç‰¹æœ‰çš„æ ‡ç‚¹ç¬¦å·
        if re.search(r'[\u3040-\u309f\u30a0-\u30ff\u3000-\u303f]', text):
            return False  # åŒ…å«æ—¥æ–‡å­—ç¬¦ï¼Œä¸æ˜¯ä¸­æ–‡
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«ä¸­æ–‡å­—ç¬¦
        chinese_chars = re.findall(r'[\u4e00-\u9fff]', text)
        if not chinese_chars:
            return False
        
        # å¦‚æœåŒ…å«ä¸­æ–‡å­—ç¬¦ï¼Œè¿›ä¸€æ­¥æ£€æŸ¥æ˜¯å¦ä¸»è¦æ˜¯ä¸­æ–‡
        # å¦‚æœä¸­æ–‡å­—ç¬¦æ•°é‡æ˜æ˜¾å¤šäºæ—¥æ–‡å­—ç¬¦ï¼Œåˆ™è®¤ä¸ºæ˜¯ä¸­æ–‡
        japanese_chars = re.findall(r'[\u3040-\u309f\u30a0-\u30ff]', text)
        return len(chinese_chars) > len(japanese_chars)
    
    def _qc_detect_lines(self, content_lines: List[Tuple[str, str]]) -> List[QCResult]:
        """
        æ‰¹é‡QCæ£€æµ‹
        
        Args:
            content_lines: åŒè¯­å†…å®¹åˆ—è¡¨
            
        Returns:
            List[QCResult]: QCæ£€æµ‹ç»“æœåˆ—è¡¨
        """
        qc_results = []
        
        # è¿‡æ»¤å‡ºéœ€è¦æ£€æµ‹çš„è¡Œ
        lines_to_check = []
        line_indices = []
        
        for i, (original, translated) in enumerate(content_lines):
            if self._contains_chinese(original):
                # è·³è¿‡çº¯ä¸­æ–‡è¡Œï¼Œç›´æ¥ç»™æ»¡åˆ†
                qc_results.append(QCResult(
                    line_index=i,
                    original_text=original,
                    translated_text=translated,
                    quality_score=1.0,
                    needs_retranslation=False,
                    reason="çº¯ä¸­æ–‡è¡Œ"
                ))
                continue
            
            lines_to_check.append((original, translated))
            line_indices.append(i)
        
        if not lines_to_check:
            return qc_results
        
        # æ‰¹é‡QCæ£€æµ‹
        try:
            scores = self._check_quality_batch_llm(lines_to_check)
            
            # å°†åˆ†æ•°æ˜ å°„å›åŸå§‹ç´¢å¼•
            for i, score in enumerate(scores):
                original_idx = line_indices[i]
                original, translated = content_lines[original_idx]
                
                # ç‰¹æ®Šå¤„ç†[ç¿»è¯‘æœªå®Œæˆ]æ ‡è®°
                if translated == "[ç¿»è¯‘æœªå®Œæˆ]":
                    needs_retranslation = True
                    quality_score = 0.0
                    reason = "æœªç¿»è¯‘"
                else:
                    needs_retranslation = score < self.config.enhanced_qc_threshold
                    quality_score = score
                    reason = f"è´¨é‡åˆ†æ•°: {score:.2f}"
                
                qc_results.append(QCResult(
                    line_index=original_idx,
                    original_text=original,
                    translated_text=translated,
                    quality_score=quality_score,
                    needs_retranslation=needs_retranslation,
                    reason=reason
                ))
                
        except Exception as e:
            self.logger.error(f"æ‰¹é‡QCæ£€æµ‹å¤±è´¥: {e}")
            # é™çº§åˆ°é€è¡Œæ£€æµ‹
            for i, (original, translated) in enumerate(lines_to_check):
                original_idx = line_indices[i]
                
                # ç‰¹æ®Šå¤„ç†[ç¿»è¯‘æœªå®Œæˆ]æ ‡è®°
                if translated == "[ç¿»è¯‘æœªå®Œæˆ]":
                    needs_retranslation = True
                    quality_score = 0.0
                    reason = "æœªç¿»è¯‘"
                else:
                    quality_score = self._llm_quality_check(original, translated)
                    needs_retranslation = quality_score < self.config.enhanced_qc_threshold
                    reason = f"è´¨é‡åˆ†æ•°: {quality_score:.2f}"
                
                qc_results.append(QCResult(
                    line_index=original_idx,
                    original_text=original,
                    translated_text=translated,
                    quality_score=quality_score,
                    needs_retranslation=needs_retranslation,
                    reason=reason
                ))
        
        return qc_results
    
    def _check_quality_batch_llm(self, lines_to_check: List[Tuple[str, str]]) -> List[float]:
        """
        ä½¿ç”¨LLMæ‰¹é‡æ£€æµ‹ç¿»è¯‘è´¨é‡
        
        Args:
            lines_to_check: éœ€è¦æ£€æµ‹çš„åŸæ–‡å’Œè¯‘æ–‡å¯¹åˆ—è¡¨
            
        Returns:
            List[float]: è´¨é‡åˆ†æ•°åˆ—è¡¨
        """
        try:
            # ä½¿ç”¨PromptBuilderæ„å»ºQCæ¶ˆæ¯
            target_lines = [original for original, _ in lines_to_check]
            translated_lines = [translated for _, translated in lines_to_check]
            
            messages = self.qc_prompt_builder.build_messages(
                target_lines=target_lines,
                translated_lines=translated_lines
            )
            
            # åŠ¨æ€è®¡ç®—max_tokensï¼Œå‚è€ƒtranslator.pyçš„é€»è¾‘
            estimated_input_tokens = self._estimate_tokens(messages)
            max_context_length = self.config.get_max_context_length()
            
            if self.config.max_tokens > 0:
                max_tokens = self.config.max_tokens
            else:
                # åŠ¨æ€è®¡ç®—max_tokens
                safety_margin = 1024
                remain = max_context_length - estimated_input_tokens - safety_margin
                if remain < 500:
                    remain = 500
                max_tokens = min(remain, 25000)  # è®¾ç½®25000çš„ä¸Šé™
            
            self.logger.info(f"QCæ£€æµ‹åŠ¨æ€è®¡ç®— max_tokens: {max_tokens} (åŸºäºè¾“å…¥tokens: {estimated_input_tokens}, æ¨¡å‹ä¸Šä¸‹æ–‡é•¿åº¦: {max_context_length})")
            
            result, token_stats = self.streaming_handler.stream_completion(
                model=self.config.model,
                messages=messages,
                temperature=0.1,
                max_tokens=max_tokens
            )
            
            # è§£æåˆ†æ•°
            scores = self._parse_qc_scores(result, len(lines_to_check))
            
            self.logger.info(f"æ‰¹é‡QCæ£€æµ‹å®Œæˆ: {len(lines_to_check)}è¡Œ, åˆ†æ•°: {scores}")
            return scores
            
        except Exception as e:
            self.logger.error(f"æ‰¹é‡QCæ£€æµ‹å¤±è´¥: {e}")
            raise
    
    def _parse_qc_scores(self, result: str, expected_count: int) -> List[float]:
        """
        è§£æQCæ£€æµ‹çš„åˆ†æ•°ç»“æœ
        
        Args:
            result: LLMè¾“å‡ºç»“æœ
            expected_count: æœŸæœ›çš„åˆ†æ•°æ•°é‡
            
        Returns:
            List[float]: è§£æå‡ºçš„åˆ†æ•°åˆ—è¡¨
        """
        # æå–çº¯å‡€çš„åˆ†æ•°ï¼ˆå»é™¤æ€è€ƒè¿‡ç¨‹ï¼‰
        clean_result = self._extract_clean_qc_scores(result)
        
        # æŒ‰è¡Œåˆ†å‰²å¹¶æå–æ•°å­—
        lines = clean_result.split('\n')
        scores = []
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # å°è¯•æå–æ•°å­—
            match = re.search(r'(\d+\.?\d*)', line)
            if match:
                try:
                    score = float(match.group(1))
                    # ç¡®ä¿åˆ†æ•°åœ¨0-1èŒƒå›´å†…
                    score = max(0.0, min(1.0, score))
                    scores.append(score)
                except ValueError:
                    continue
        
        # å¦‚æœåˆ†æ•°æ•°é‡ä¸å¤Ÿï¼Œç”¨é»˜è®¤åˆ†æ•°å¡«å……
        while len(scores) < expected_count:
            scores.append(0.5)  # é»˜è®¤ä¸­ç­‰åˆ†æ•°
        
        # å¦‚æœåˆ†æ•°æ•°é‡è¿‡å¤šï¼Œæˆªå–å‰é¢çš„
        if len(scores) > expected_count:
            scores = scores[:expected_count]
        
        return scores
    
    def _extract_clean_qc_scores(self, result: str) -> str:
        """
        ä»LLMè¾“å‡ºä¸­æå–çº¯å‡€çš„QCåˆ†æ•°
        
        Args:
            result: LLMåŸå§‹è¾“å‡º
            
        Returns:
            str: çº¯å‡€çš„åˆ†æ•°æ–‡æœ¬
        """
        # ç§»é™¤æ€è€ƒæ ‡ç­¾
        clean_result = re.sub(r'<think>.*?</think>', '', result, flags=re.DOTALL)
        
        # ç§»é™¤å…¶ä»–æ ‡ç­¾
        clean_result = re.sub(r'<[^>]+>', '', clean_result)
        
        # ç§»é™¤å¸¸è§çš„å¯¹è¯æ ‡è®°
        clean_result = re.sub(r'\[ç¿»è¯‘å®Œæˆ\]', '', clean_result)
        clean_result = re.sub(r'\[END\]', '', clean_result)
        
        return clean_result.strip()
    
    def _estimate_tokens(self, messages: List[dict]) -> int:
        """
        ä¼°ç®—æ¶ˆæ¯çš„tokenæ•°é‡
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            
        Returns:
            int: ä¼°ç®—çš„tokenæ•°é‡
        """
        total_chars = 0
        for message in messages:
            if isinstance(message, dict) and 'content' in message:
                total_chars += len(message['content'])
        
        # ç²—ç•¥ä¼°ç®—ï¼šä¸­æ–‡çº¦1.5å­—ç¬¦/tokenï¼Œè‹±æ–‡çº¦4å­—ç¬¦/token
        # è¿™é‡Œä½¿ç”¨ä¿å®ˆä¼°ç®—ï¼š2å­—ç¬¦/token
        estimated_tokens = total_chars // 2
        
        # æ·»åŠ ä¸€äº›ä½™é‡
        return int(estimated_tokens * 1.2)
    
    def _llm_quality_check(self, original: str, translated: str) -> float:
        """
        ä½¿ç”¨LLMè¿›è¡Œè´¨é‡æ£€æµ‹ï¼ˆé€è¡Œï¼‰
        
        Args:
            original: åŸæ–‡
            translated: è¯‘æ–‡
            
        Returns:
            float: è´¨é‡åˆ†æ•° (0-1)
        """
        try:
            # ä½¿ç”¨ç»Ÿä¸€çš„promptæ„å»ºæ–¹æ³•
            messages = self._build_qc_messages([(original, translated)])
            
            # åŠ¨æ€è®¡ç®—max_tokens
            estimated_input_tokens = self._estimate_tokens(messages)
            max_context_length = self.config.get_max_context_length()
            
            if self.config.max_tokens > 0:
                max_tokens = self.config.max_tokens
            else:
                safety_margin = 1024
                remain = max_context_length - estimated_input_tokens - safety_margin
                if remain < 500:
                    remain = 500
                max_tokens = min(remain, 25000)
            
            result, token_stats = self.streaming_handler.stream_completion(
                model=self.config.model,
                messages=messages,
                temperature=0.1,
                max_tokens=max_tokens
            )
            
            # è§£æåˆ†æ•°
            scores = self._parse_qc_scores(result, 1)
            return scores[0] if scores else 0.5
                
        except Exception as e:
            self.logger.error(f"LLMè´¨é‡æ£€æµ‹å¤±è´¥: {e}")
            return 0.5  # é»˜è®¤ä¸­ç­‰åˆ†æ•°
    
    def _retranslate_lines(self, content_lines: List[Tuple[str, str]], 
                          needs_retranslation: List[QCResult]) -> Dict[int, str]:
        """
        é‡æ–°ç¿»è¯‘è´¨é‡ä¸ä½³çš„è¡Œ
        
        Args:
            content_lines: æ‰€æœ‰åŒè¯­å†…å®¹
            needs_retranslation: éœ€è¦é‡æ–°ç¿»è¯‘çš„è¡Œ
            
        Returns:
            Dict[int, str]: {è¡Œç´¢å¼•: æ–°è¯‘æ–‡}
        """
        retranslated = {}
        
        for qc_result in needs_retranslation:
            line_index = qc_result.line_index
            original = qc_result.original_text
            
            # è·å–ä¸Šä¸‹æ–‡
            context_lines = self._get_context_lines(content_lines, line_index)
            
            # é‡æ–°ç¿»è¯‘
            new_translation = self._retranslate_single_line(original, context_lines)
            
            if new_translation:
                retranslated[line_index] = new_translation
                self.logger.info(f"é‡æ–°ç¿»è¯‘å®Œæˆ: è¡Œ{line_index+1}")
            else:
                self.logger.warning(f"é‡æ–°ç¿»è¯‘å¤±è´¥: è¡Œ{line_index+1}")
        
        return retranslated

    def _enhance_all_lines_batch(self, content_lines: List[Tuple[str, str]], original_lines: List[str], target_path: Path) -> Dict[int, str]:
        """ç›´æ¥å¢å¼ºæ‰€æœ‰è¡Œï¼šè®©æ¨¡å‹æ£€æŸ¥å¹¶æ”¹è¿›æ‰€æœ‰è¡Œ"""
        enhanced: Dict[int, str] = {}
        if not content_lines:
            return enhanced
        
        batch_size = max(1, int(getattr(self.config, 'enhanced_batch_size', 10)))
        
        # ç»„æ‰¹å¤„ç†æ‰€æœ‰è¡Œ
        for start in range(0, len(content_lines), batch_size):
            end = min(start + batch_size, len(content_lines))
            batch_lines = content_lines[start:end]
            
            # æ„å»ºprevious_ioï¼ˆé™¤äº†ç¬¬ä¸€æ‰¹ï¼‰
            previous_io = None
            if start > 0:
                # è·å–å‰ä¸€æ‰¹çš„è¾“å‡ºä½œä¸ºprevious_io
                prev_start = max(0, start - batch_size)
                prev_end = start
                prev_batch_lines = content_lines[prev_start:prev_end]
                prev_output_lines = [enhanced.get(prev_start + i, "") for i in range(len(prev_batch_lines))]
                # åªæœ‰å½“æ‰€æœ‰å‰ä¸€æ‰¹çš„è¾“å‡ºéƒ½å­˜åœ¨æ—¶æ‰æ„å»ºprevious_io
                if prev_output_lines and all(prev_output_lines):
                    previous_io = (
                        [original for original, _ in prev_batch_lines],  # input_lines
                        prev_output_lines  # output_lines
                    )
            
            # æ„å»ºå¢å¼ºæ¶ˆæ¯
            messages = self._build_enhance_all_messages(batch_lines, previous_io)
            
            try:
                result, token_stats = self.streaming_handler.stream_completion(
                    model=self.config.model,
                    messages=messages,
                    temperature=0.0,
                    top_p=1.0,
                    frequency_penalty=0.0,
                    presence_penalty=0.0,
                    repetition_penalty=1.0,
                    no_repeat_ngram_size=0,
                    max_tokens=2048,
                    stop=["ï¼ˆæœªå®Œå¾…ç»­ï¼‰", "[END]", "<|im_end|>", "</s>"]
                )
                
                # æ£€æŸ¥finish_reasonï¼Œå¦‚æœæ˜¯lengthåˆ™é™çº§å¤„ç†
                finish_reason = token_stats.get('finish_reason', 'unknown')
                if finish_reason == 'length':
                    self.logger.warning(f"âš ï¸ æ¨¡å‹è¾“å‡ºè¢«æˆªæ–­ (length)ï¼Œé™çº§åˆ°é€è¡Œå¤„ç†: è¡Œ {start+1}-{end}")
                    # é™çº§åˆ°é€è¡Œå¤„ç†
                    for i, (original, translated) in enumerate(batch_lines):
                        try:
                            enhanced[start + i] = self._enhance_single_line(original, translated)
                        except Exception as single_e:
                            self.logger.error(f"å•è¡Œå¢å¼ºå¤±è´¥ (è¡Œ{start + i}): {single_e}")
                            enhanced[start + i] = translated  # ä¿æŒåŸè¯‘æ–‡
                    continue
                
                # è§£æå¤šè¡Œè¾“å‡º
                cleaned = self._extract_clean_translation(result)
                out_lines = [l.strip() for l in cleaned.split('\n') if l.strip()]
                
                # å°†è¾“å‡ºæ˜ å°„å›åŸå§‹ç´¢å¼•
                for idx, line in enumerate(out_lines):
                    if idx < len(batch_lines):
                        enhanced[start + idx] = line
                
                self.logger.info(f"âœ… æ‰¹æ¬¡å®Œæˆ: è¡Œ {start+1}-{end}ï¼Œå·²å¤„ç†{len(out_lines)}è¡Œ")
                
                # æ¯æ‰¹æ¬¡å†™ç›˜
                try:
                    self._update_bilingual_file(target_path, original_lines, content_lines, enhanced)
                    self.logger.info(f"æ–‡ä»¶æ›´æ–°å®Œæˆ: {target_path}, æ›´æ–°äº†{len(enhanced)}è¡Œ")
                    # å®‰å…¨åœ°è·å–æ—¥å¿—æ–‡ä»¶è·¯å¾„
                    log_file_path = 'N/A'
                    if hasattr(self.logger, 'handlers') and self.logger.handlers:
                        try:
                            log_file_path = self.logger.handlers[0].baseFilename
                        except (AttributeError, IndexError):
                            pass
                    elif hasattr(self.logger, 'log_file_path'):
                        log_file_path = self.logger.log_file_path
                    self.logger.info(f"   ğŸ“ æ—¥å¿—æ–‡ä»¶: {log_file_path}")
                    self.logger.info(f"   ğŸ“„ è¾“å‡ºæ–‡ä»¶: {target_path}")
                    self.logger.info(f"   ğŸ”¢ Tokenä½¿ç”¨: {token_stats}")
                except Exception as e:
                    self.logger.error(f"æ–‡ä»¶æ›´æ–°å¤±è´¥: {e}")
                
            except Exception as e:
                self.logger.error(f"æ‰¹æ¬¡å¢å¼ºå¤±è´¥: {e}")
                # é™çº§åˆ°é€è¡Œå¤„ç†
                for i, (original, translated) in enumerate(batch_lines):
                    try:
                        enhanced[start + i] = self._enhance_single_line(original, translated)
                    except Exception as single_e:
                        self.logger.error(f"å•è¡Œå¢å¼ºå¤±è´¥ (è¡Œ{start + i}): {single_e}")
                        enhanced[start + i] = translated  # ä¿æŒåŸè¯‘æ–‡
        
        return enhanced
    
    def _build_enhance_all_messages(self, batch_lines: List[Tuple[str, str]], previous_io: Optional[Tuple[List[str], List[str]]] = None) -> List[Dict[str, str]]:
        """æ„å»ºå¢å¼ºæ‰€æœ‰è¡Œçš„æ¶ˆæ¯"""
        # å¯¼å…¥è§„åˆ™æ£€æµ‹æ¨¡å—
        from .rule_detector import detect_translation_issues, format_issues_for_enhancement
        
        # æ£€æµ‹è§„åˆ™é—®é¢˜
        rule_issues = []
        for original, translated in batch_lines:
            issues = detect_translation_issues(original, translated)
            formatted_issues = format_issues_for_enhancement(issues)
            rule_issues.append(formatted_issues)
        
        messages = self.enhancement_prompt_builder.build_messages(
            target_lines=[original for original, _ in batch_lines],
            translated_lines=[translated for _, translated in batch_lines],
            previous_io=previous_io,
            rule_issues=rule_issues
        )
        return messages
    
    def _enhance_single_line(self, original: str, translated: str) -> str:
        """å•è¡Œå¢å¼ºï¼ˆé™çº§å¤„ç†ï¼‰"""
        # å¯¼å…¥è§„åˆ™æ£€æµ‹æ¨¡å—
        from .rule_detector import detect_translation_issues, format_issues_for_enhancement
        
        # æ£€æµ‹è§„åˆ™é—®é¢˜
        issues = detect_translation_issues(original, translated)
        formatted_issues = format_issues_for_enhancement(issues)
        
        messages = self.enhancement_prompt_builder.build_messages(
            target_lines=[original],
            translated_lines=[translated],
            rule_issues=[formatted_issues]
        )
        
        result, _ = self.streaming_handler.stream_completion(
            model=self.config.model,
            messages=messages,
            temperature=0.0,
            max_tokens=512
        )
        
        cleaned = self._extract_clean_translation(result)
        lines = [l.strip() for l in cleaned.split('\n') if l.strip()]
        return lines[0] if lines else translated

    def _retranslate_lines_batch(self, content_lines: List[Tuple[str, str]], 
                          needs_retranslation: List[QCResult], original_lines: List[str], target_path: Path) -> Dict[int, str]:
        """æ‰¹é‡é‡è¯‘ï¼šä¸€æ¬¡é€å…¥Nä¸ªåŸ/è¯‘å¯¹ï¼Œè®©æ¨¡å‹é€è¡Œè¿”å›æ”¹å†™åçš„ä¸­æ–‡"""
        retranslated: Dict[int, str] = {}
        if not needs_retranslation:
            return retranslated
        batch_size = max(1, int(getattr(self.config, 'enhanced_batch_size', 10)))
        # ç»„æ‰¹
        for start in range(0, len(needs_retranslation), batch_size):
            batch = needs_retranslation[start:start+batch_size]
            # æ„å»ºæ‰¹é‡æç¤ºï¼šå‚è€ƒbilingual_simpleçš„å¤šè½®å¯¹è¯æ ¼å¼
            messages = self._build_enhanced_messages(batch)
            indices = [item.line_index for item in batch]
            try:
                result, token_stats = self.streaming_handler.stream_completion(
                    model=self.config.model,
                    messages=messages,
                    temperature=0.0,
                    top_p=1.0,
                    frequency_penalty=0.0,
                    presence_penalty=0.0,
                    repetition_penalty=1.0,
                    no_repeat_ngram_size=0,
                    max_tokens=2048,
                    stop=["ï¼ˆæœªå®Œå¾…ç»­ï¼‰", "[END]", "<|im_end|>", "</s>"]
                )
                # è§£æå¤šè¡Œè¾“å‡º
                cleaned = self._extract_clean_translation(result)
                # å…è®¸å¤šè¡Œï¼ŒæŒ‰è¡Œæ‹†åˆ†
                out_lines = [l.strip() for l in cleaned.split('\n') if l.strip()]
                # è‹¥è¡Œæ•°ä¸åŒ¹é…ï¼Œåˆ™å°½é‡å¯¹é½è¾ƒçŸ­éƒ¨åˆ†
                for idx, line_index in enumerate(indices):
                    if idx < len(out_lines) and out_lines[idx]:
                        retranslated[line_index] = out_lines[idx]
                batch_first = indices[0] + 1
                batch_last = indices[0] + len(indices)
                self.logger.info(f"âœ… æ‰¹æ¬¡å®Œæˆ: è¡Œ {batch_first}-{batch_last}ï¼Œå·²æ”¹å†™{len(out_lines)}è¡Œ")
                # æ¯æ‰¹æ¬¡å†™ç›˜å¹¶æ‰“å°è·¯å¾„ï¼ˆå¯¹é½ bilingual_simple çš„è¡Œä¸ºï¼‰
                try:
                    self._update_bilingual_file(target_path, original_lines, content_lines, retranslated)
                except Exception as e:
                    self.logger.error(f"æ‰¹æ¬¡å†™ç›˜å¤±è´¥: {e}")
                # æç¤ºè·¯å¾„åˆ°æ§åˆ¶å°
                log_path = self.logger.get_log_file_path() if hasattr(self.logger, 'get_log_file_path') else None
                self.logger.info(f"   ğŸ“ æ—¥å¿—æ–‡ä»¶: {log_path}", mode=UnifiedLogger.LogMode.BOTH)
                self.logger.info(f"   ğŸ“„ è¾“å‡ºæ–‡ä»¶: {target_path}", mode=UnifiedLogger.LogMode.BOTH)
                if token_stats:
                    self.logger.info(f"   ğŸ”¢ Tokenä½¿ç”¨: {token_stats}", mode=UnifiedLogger.LogMode.BOTH)
            except Exception as e:
                self.logger.error(f"æ‰¹é‡é‡è¯‘å¤±è´¥: {e}")
                # é™çº§ï¼šé€è¡Œé‡è¯‘
                fallback = self._retranslate_lines(content_lines, batch)
                retranslated.update(fallback)
        return retranslated
    
    def _build_enhanced_messages(self, batch: List[QCResult]) -> List[Dict[str, str]]:
        """
        æ„å»ºå¢å¼ºæ¨¡å¼çš„å¤šè½®å¯¹è¯æ¶ˆæ¯ï¼ˆå‚è€ƒbilingual_simpleæ ¼å¼ï¼‰
        
        Args:
            batch: éœ€è¦é‡æ–°ç¿»è¯‘çš„QCç»“æœåˆ—è¡¨
            
        Returns:
            List[Dict[str, str]]: å¤šè½®å¯¹è¯æ¶ˆæ¯
        """
        # ä½¿ç”¨PromptBuilderæ„å»ºå¢å¼ºæ¶ˆæ¯
        target_lines = [qc_result.original_text for qc_result in batch]
        translated_lines = [qc_result.translated_text for qc_result in batch]
        
        messages = self.enhancement_prompt_builder.build_messages(
            target_lines=target_lines,
            translated_lines=translated_lines
        )
        
        return messages
    def _get_context_lines(self, content_lines: List[Tuple[str, str]], 
                          target_index: int) -> List[Tuple[str, str]]:
        """è·å–ç›®æ ‡è¡Œçš„ä¸Šä¸‹æ–‡"""
        context_size = self.config.enhanced_context_lines
        start = max(0, target_index - context_size)
        end = min(len(content_lines), target_index + context_size + 1)
        
        return content_lines[start:end]
    
    def _retranslate_single_line(self, original: str, 
                                context_lines: List[Tuple[str, str]]) -> Optional[str]:
        """
        é‡æ–°ç¿»è¯‘å•è¡Œ
        
        Args:
            original: åŸæ–‡
            context_lines: ä¸Šä¸‹æ–‡è¡Œ
            
        Returns:
            Optional[str]: æ–°è¯‘æ–‡ï¼Œå¤±è´¥æ—¶è¿”å›None
        """
        try:
            # æ„å»ºä¸Šä¸‹æ–‡
            context_text = ""
            for orig, trans in context_lines:
                context_text += f"åŸæ–‡: {orig}\nè¯‘æ–‡: {trans}\n"
            
            # ä½¿ç”¨å¢å¼ºæ¨¡å¼çš„ç³»ç»Ÿæç¤ºè¯
            preface_path = Path(__file__).parent.parent.parent / "data" / "preface_enhanced.txt"
            if preface_path.exists():
                with open(preface_path, 'r', encoding='utf-8') as f:
                    system_content = f.read().strip()
            else:
                system_content = "ä½ æ˜¯ä¸“ä¸šçš„ä¸­æ—¥äº’è¯‘ç¼–è¾‘ã€‚ç»™å®šåŸæ–‡ä¸å½“å‰è¯‘æ–‡ï¼Œè¯·æ”¹è¿›è´¨é‡ï¼Œä»…è¾“å‡ºæ”¹è¿›åçš„ä¸­æ–‡è¯‘æ–‡ï¼Œä¸è¦ä»»ä½•è§£é‡Šã€‚"
            
            messages = [
                {"role": "system", "content": system_content},
                {"role": "user", "content": f"åŸæ–‡: {original}\nç°è¯‘: {context_lines[0][1] if context_lines else '[ç¿»è¯‘æœªå®Œæˆ]'}\n[ç¿»è¯‘å®Œæˆ]"}
            ]
            
            result, token_stats = self.streaming_handler.stream_completion(
                model=self.config.model,
                messages=messages,
                temperature=0.0,
                top_p=1.0,
                frequency_penalty=0.0,
                presence_penalty=0.0,
                repetition_penalty=1.0,
                no_repeat_ngram_size=0,
                max_tokens=1024,
                stop=["ï¼ˆæœªå®Œå¾…ç»­ï¼‰", "[END]", "<|im_end|>", "</s>"]
            )
            
            # æå–çº¯å‡€çš„ç¿»è¯‘ç»“æœï¼ˆå»é™¤æ€è€ƒè¿‡ç¨‹ï¼‰
            clean_result = self._extract_clean_translation(result)
            self.logger.info(f"è°ƒè¯•: åŸå§‹ç»“æœ={repr(result[:100])}, æ¸…ç†åç»“æœ={repr(clean_result)}")
            return clean_result
            
        except Exception as e:
            self.logger.error(f"é‡æ–°ç¿»è¯‘å¤±è´¥: {e}")
            return None
    
    def _extract_clean_translation(self, result: str) -> str:
        """
        ä»æ¨¡å‹è¾“å‡ºä¸­æå–çº¯å‡€çš„ç¿»è¯‘ç»“æœ
        
        Args:
            result: æ¨¡å‹åŸå§‹è¾“å‡º
            
        Returns:
            str: çº¯å‡€çš„ç¿»è¯‘ç»“æœ
        """
        # å»é™¤é¦–å°¾ç©ºç™½
        result = result.strip()
        
        # å¦‚æœç»“æœä¸ºç©ºï¼Œè¿”å›åŸç»“æœ
        if not result:
            return result
        
        import re
        
        # å»é™¤<think>æ ‡ç­¾åŠå…¶å†…å®¹ï¼ˆå¤„ç†æ²¡æœ‰é—­åˆæ ‡ç­¾çš„æƒ…å†µï¼‰
        result = re.sub(r'<think>.*?</think>', '', result, flags=re.DOTALL)
        result = re.sub(r'<think>.*', '', result, flags=re.DOTALL)
        
        # å»é™¤å…¶ä»–å¸¸è§çš„æ€è€ƒæ ‡è®°
        result = re.sub(r'<thinking>.*?</thinking>', '', result, flags=re.DOTALL)
        result = re.sub(r'<reasoning>.*?</reasoning>', '', result, flags=re.DOTALL)
        
        # å»é™¤é¦–å°¾ç©ºç™½
        result = result.strip()
        
        # å¦‚æœæ¸…ç†åç»“æœä¸ºç©ºï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²
        if not result:
            return ""
        
        # æŒ‰è¡Œåˆ†å‰²ï¼Œå¤„ç†å¸¦ç¼–å·çš„å¤šè¡Œè¾“å‡º
        lines = result.split('\n')
        clean_lines = []
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # ç§»é™¤è¡Œå·ï¼ˆå¦‚ "1. è¯‘æ–‡å†…å®¹" -> "è¯‘æ–‡å†…å®¹"ï¼‰
            if re.match(r'^\d+\.\s*', line):
                line = re.sub(r'^\d+\.\s*', '', line)
            
            # å¤„ç†å¢å¼ºæ¨¡å¼çš„ç®­å¤´æ ¼å¼ï¼ˆå¦‚ "â†’ è¯‘æ–‡å†…å®¹" -> "è¯‘æ–‡å†…å®¹"ï¼‰
            if line.startswith('â†’'):
                line = line[1:].strip()
            
            # è·³è¿‡[ç¿»è¯‘å®Œæˆ]æ ‡è®°
            if line == "[ç¿»è¯‘å®Œæˆ]":
                continue
            
            # è·³è¿‡å…¶ä»–æ ‡è®°
            if line.startswith('[') and line.endswith(']'):
                continue
            
            # è·³è¿‡æ˜æ˜¾çš„æ€è€ƒå†…å®¹ï¼ˆä½†ä¿ç•™ç¿»è¯‘å†…å®¹ï¼‰
            # åªæœ‰å½“æ•´è¡Œéƒ½æ˜¯æ€è€ƒå†…å®¹æ—¶æ‰è·³è¿‡ï¼Œä¸è¦è·³è¿‡åŒ…å«å…³é”®è¯çš„ç¿»è¯‘
            if (line.startswith('å¥½çš„') or line.startswith('ç”¨æˆ·') or 
                line.startswith('è®©æˆ‘') or line.startswith('ç¿»è¯‘') or
                line.startswith('é¦–å…ˆ') or line.startswith('éœ€è¦') or
                line.startswith('ç¡®è®¤') or line.startswith('æ¥ä¸‹æ¥') or
                line.startswith('ç„¶å') or line.startswith('å¦å¤–') or
                line.startswith('æœ€å') or line.startswith('æ£€æŸ¥') or
                line.startswith('ç¡®ä¿') or line.startswith('å¯èƒ½') or
                line.startswith('åº”è¯¥') or line.startswith('ä¸è¿‡') or
                line.startswith('ä½†æ˜¯') or line.startswith('å› ä¸º') or
                line.startswith('å¦‚æœ') or line.startswith('è™½ç„¶') or
                line.startswith('æ ¹æ®') or line.startswith('è€ƒè™‘') or
                line.startswith('æ³¨æ„')):
                continue
            
            # å¦‚æœè¿™è¡Œçœ‹èµ·æ¥åƒç¿»è¯‘ç»“æœï¼Œæ·»åŠ åˆ°ç»“æœä¸­
            if len(line) > 0:
                clean_lines.append(line)
        
        # è¿”å›æ‰€æœ‰æœ‰æ•ˆè¡Œï¼Œç”¨æ¢è¡Œç¬¦è¿æ¥
        if clean_lines:
            return '\n'.join(clean_lines)
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å¹²å‡€çš„è¡Œï¼Œå°è¯•æå–å¼•å·å†…çš„å†…å®¹
        quoted_matches = re.findall(r'ã€Œ([^ã€]+)ã€', result)
        if quoted_matches:
            return quoted_matches[-1]
        
        # å¦‚æœéƒ½æ²¡æœ‰ï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²
        return ""
    
    def _update_bilingual_file(self, file_path: Path, original_lines: List[str],
                              content_lines: List[Tuple[str, str]], 
                              retranslated_lines: Dict[int, str]):
        """
        æ›´æ–°åŒè¯­æ–‡ä»¶
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            original_lines: åŸå§‹æ–‡ä»¶è¡Œ
            content_lines: åŒè¯­å†…å®¹
            retranslated_lines: é‡æ–°ç¿»è¯‘çš„è¡Œ
        """
        if not retranslated_lines:
            return
        
        self.logger.info(f"è°ƒè¯•ä¿¡æ¯: content_lines={len(content_lines)}, retranslated_lines={retranslated_lines}")
        
        # æ‰¾åˆ°YAMLç»“æŸä½ç½®ï¼šè·³è¿‡æ‰€æœ‰YAMLå†…å®¹ç›´åˆ°ç¬¬äºŒä¸ª---æˆ–å®é™…å†…å®¹å¼€å§‹
        yaml_end = 0
        yaml_started = False
        while yaml_end < len(original_lines):
            line = original_lines[yaml_end].strip()
            if line.startswith('---'):
                if not yaml_started:
                    yaml_started = True
                else:
                    # æ‰¾åˆ°ç¬¬äºŒä¸ª---ï¼ŒYAMLç»“æŸ
                    yaml_end += 1
                    break
            elif yaml_started and not line.startswith('---'):
                # åœ¨YAMLä¸­ï¼Œç»§ç»­è·³è¿‡
                pass
            elif not yaml_started and line and not line.startswith('---'):
                # æ²¡æœ‰YAMLï¼Œç›´æ¥å¼€å§‹è§£æå†…å®¹
                break
            yaml_end += 1
        
        # é‡æ–°æ„å»ºæ–‡ä»¶å†…å®¹
        new_lines = original_lines[:yaml_end]  # ä¿ç•™YAMLéƒ¨åˆ†
        
        # åˆ›å»ºå†…å®¹è¡Œç´¢å¼•æ˜ å°„
        content_index = 0
        
        # ä»YAMLç»“æŸåå¼€å§‹å¤„ç†åŸå§‹å†…å®¹
        i = yaml_end
        while i < len(original_lines):
            line = original_lines[i].strip()
            
            # å¦‚æœæ˜¯ç©ºè¡Œï¼Œä¿ç•™
            if not line:
                new_lines.append('\n')
                i += 1
                continue
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯åŸæ–‡è¡Œï¼ˆä¸åŒ…å«ä¸­æ–‡ï¼‰
            if not self._contains_chinese(line) and content_index < len(content_lines):
                original, translated = content_lines[content_index]
                
                # å¦‚æœè¿™ä¸€è¡Œéœ€è¦é‡æ–°ç¿»è¯‘ï¼Œä½¿ç”¨æ”¹è¿›åçš„è¯‘æ–‡
                if content_index in retranslated_lines:
                    translated = retranslated_lines[content_index]
                # å¦åˆ™ï¼Œå¦‚æœè¿™ä¸€è¡Œä¹‹å‰å·²ç»è¢«æ”¹è¿›è¿‡ï¼Œä½¿ç”¨æ”¹è¿›åçš„è¯‘æ–‡
                elif content_index in self.previous_improvements:
                    translated = self.previous_improvements[content_index]
                
                # æ·»åŠ åŸæ–‡å’Œè¯‘æ–‡
                new_lines.append(original + '\n')
                new_lines.append(translated + '\n')
                
                content_index += 1
                i += 2  # è·³è¿‡åŸæ–‡å’Œè¯‘æ–‡è¡Œ
            else:
                # ä¿ç•™å…¶ä»–è¡Œï¼ˆå¦‚ç©ºè¡Œã€æ ¼å¼è¡Œç­‰ï¼‰
                new_lines.append(original_lines[i])
                i += 1
        
        # å†™å›æ–‡ä»¶
        with open(file_path, 'w', encoding='utf-8') as f:
            f.writelines(new_lines)
        
        # æ›´æ–°ä¹‹å‰çš„æ”¹è¿›è®°å½•
        self.previous_improvements.update(retranslated_lines)
        
        self.logger.info(f"æ–‡ä»¶æ›´æ–°å®Œæˆ: {file_path}, æ›´æ–°äº†{len(retranslated_lines)}è¡Œ")

    def _resolve_output_path(self, file_path: Path) -> Path:
        """æ ¹æ®é…ç½®ä¸ debug è§„åˆ™ï¼Œè§£æå¢å¼ºè¾“å‡ºæ–‡ä»¶è·¯å¾„"""
        if self.config.enhanced_output == 'inplace':
            return file_path
        # copy æ¨¡å¼
        if self.config.debug:
            # åŒç›®å½• {original}_enhanced.txt
            return file_path.with_name(f"{file_path.stem}_enhanced{file_path.suffix}")
        # é debug: {folder}_enhanced/{original}.txt
        enhanced_dir = file_path.parent.with_name(file_path.parent.name + "_enhanced")
        enhanced_dir.mkdir(parents=True, exist_ok=True)
        return enhanced_dir / file_path.name
