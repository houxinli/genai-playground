SHELL := /usr/bin/bash

CONDA_ENV := llm
PY := conda run -n $(CONDA_ENV) python
PIP := conda run -n $(CONDA_ENV) python -m pip

.PHONY: test-gpu deps-llm vllm serve-vllm chat-vllm safe-test-gpu safe-show-vllm safe-report-sizes env-setup env-export

# çŽ¯å¢ƒç®¡ç†
env-setup:
	@echo "ðŸš€ è®¾ç½® vLLM è¿è¡ŒçŽ¯å¢ƒ..."
	source scripts/env_setup.sh

env-export:
	@echo "ðŸ“¦ å¯¼å‡ºçŽ¯å¢ƒé…ç½®..."
	conda env export -n $(CONDA_ENV) > environment-llm.yml
	$(PIP) freeze > requirements-llm.txt
	@echo "âœ… çŽ¯å¢ƒé…ç½®å·²å¯¼å‡ºåˆ° environment-llm.yml å’Œ requirements-llm.txt"

# çŽ°æœ‰å‘½ä»¤
test-gpu:
	$(PY) scripts/test_gpu.py

safe-test-gpu:
	./scripts/run_safe.sh $(PY) scripts/test_gpu.py

safe-show-vllm:
	./scripts/run_safe.sh $(PY) -m pip show vllm

safe-report-sizes:
	./scripts/run_safe.sh $(PY) scripts/report_sizes.py

deps-llm:
	$(PIP) install -U pip wheel setuptools
	$(PIP) install -U torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124
	$(PIP) install -U transformers accelerate datasets peft sentencepiece tiktoken evaluate bitsandbytes

vllm:
	# è¯´æ˜Žï¼šé»˜è®¤ä¸è£…ä¾èµ–(--no-deps)ï¼Œä»¥é¿å… vLLM å¼ºåˆ¶å‡çº§/æ›¿æ¢å·²è£…çš„ PyTorch 2.6.0+cu124ã€‚
	# ç­–ç•¥ï¼šå…ˆå°è¯•ä¸»çº¿ç‰ˆæœ¬èŒƒå›´ï¼Œå¤±è´¥æ—¶å†å›žé€€åˆ°å…¼å®¹ Torch 2.6 çš„ç‰ˆæœ¬ã€‚
	$(PIP) install -U "vllm>=0.9.0,<0.11.0" --no-deps || \
	$(PIP) install -U vllm==0.6.2 --no-deps

serve-vllm:
	@echo "ðŸš€ å¯åŠ¨ vLLM æœåŠ¡..."
	@echo "ðŸ’¡ æç¤ºï¼šé¦–æ¬¡è¿è¡Œå‰è¯·å…ˆæ‰§è¡Œ 'make env-setup'"
	./scripts/serve_vllm.sh

chat-vllm:
	$(PY) scripts/chat_vllm.py


