# 2024-08-28 项目启动

## 🎯 项目目标
- 在 4× RTX 6000 服务器上系统体验 LLM 推理与微调
- 掌握核心工程栈（vLLM、HF 生态、LoRA/QLoRA、分布式/并行概念）
- 建立完整的本地机器翻译服务

## 🏗️ 初始规划

### 阶段1：vLLM 推理与服务
- **目标**：部署主流 7B 指令模型
- **内容**：模型加载、API 服务、性能测试
- **时间**：1-2周

### 阶段2：指令模型加载与推理
- **目标**：transformers vs vLLM 对比
- **内容**：性能对比、功能对比、使用场景分析
- **时间**：1周

### 阶段3：QLoRA 微调最小闭环
- **目标**：小样本微调实验
- **内容**：数据准备、微调训练、效果评估
- **时间**：2-3周

### 阶段4：评估与部署
- **目标**：OpenAI 兼容接口、并发、流式
- **内容**：接口设计、并发测试、流式输出
- **时间**：1-2周

### 阶段5：扩展功能
- **目标**：VLM 与视觉生成/微调（可选）
- **内容**：视觉模型集成、图像生成、视觉微调
- **时间**：2-4周

## 📚 技术背景学习

### vLLM 接口能力
- **OpenAI 兼容 API**：`/v1/chat/completions`，支持流式、并发、温度等参数
- **原生 Python**：`vllm.LLM` 类，直接加载模型进行推理
- **命令行**：`vllm.inference` 用于单次推理
- **批处理**：`vllm.batch` 用于批量推理

### 为什么使用 OpenAI 兼容接口？
- **生态兼容**：已有代码/工具无需修改
- **标准化**：统一的参数格式和响应结构
- **部署便利**：可直接替换 OpenAI 服务

### 模型排行榜
- **Hugging Face Open LLM Leaderboard**：开源模型性能对比
- **LMSYS Chatbot Arena**：聊天机器人对战排名
- **当前最强 7B**：Qwen2.5-7B-Instruct、Llama-3.1-8B-Instruct

### 硬件要求
- **推理 7B**：单张 RTX 6000 即可（量化后）
- **训练 7B**：
  - 全参数：8× A100 80GB+
  - LoRA/QLoRA：1-2张 RTX 6000
- **M1 Pro MacBook**：可运行量化后的 7B（llama.cpp、MLX、ollama）

## 🔧 环境配置

### 初始环境状态
- **驱动/CUDA**：`NVIDIA-SMI 570.124.06`, CUDA runtime 12.8
- **GPU**：4× RTX 6000 Ada, 49GB 显存
- **PyTorch**：2.6.0+cu124 已安装

### 环境配置步骤
1. **创建 conda 环境**：`llm` 环境（Python 3.10）
2. **安装 PyTorch**：cu124、transformers/accelerate/datasets/peft/bitsandbytes
3. **安装 vLLM**：与已装 PyTorch 匹配

### 配置验证
```bash
# 检查 CUDA 环境
nvidia-smi
python -c "import torch; print(torch.cuda.is_available())"

# 检查 vLLM 安装
python -c "import vllm; print(vllm.__version__)"

# 检查 transformers
python -c "import transformers; print(transformers.__version__)"
```

## 🎯 小测·第1组（vLLM 基础）

### 理论问题
1. **问**：vLLM 相比 transformers.pipeline 推理的主要优势有哪些？适合哪些场景？
2. **问**：在多 GPU 上，vLLM 的 `--tensor-parallel-size` 起什么作用？与显存/吞吐的关系？
3. **问**：KV Cache 是什么？为何能大幅提升长对话/长上下文推理吞吐？

### 实操任务
4. **实操**：启动 vLLM（7B 指令模型），用 `scripts/chat_vllm.py` 访问一次

> **作答区**（待完成）

## 🔍 问题解决与突破

### 问题1: CUDA 库链接问题
- **问题**：`cannot find -lcuda` 编译错误
- **解决**：用户级符号链接 + 环境变量设置
- **经验**：避免系统级修改，优先用户级解决方案

### 问题2: 服务管理优化
- **问题**：需要前台运行 + 日志记录
- **解决**：时间戳日志 + 管理脚本
- **经验**：使用 `tee` 命令同时输出到控制台和文件

### 问题3: 文件结构整理
- **问题**：文件位置混乱，路径错误
- **解决**：按功能模块组织，统一路径规范
- **经验**：始终在项目目录内创建文件

## 📋 学习计划

### 短期学习目标（1-2周）
1. **vLLM 基础**：掌握 vLLM 的基本使用和配置
2. **模型加载**：理解不同模型的加载方式和参数
3. **API 服务**：建立稳定的 API 服务
4. **性能测试**：进行基本的性能测试和优化

### 中期学习目标（1个月）
1. **微调实验**：尝试 QLoRA 微调
2. **并发处理**：实现多请求并发处理
3. **流式输出**：支持流式响应
4. **质量评估**：建立翻译质量评估机制

### 长期学习目标（2-3个月）
1. **多模型支持**：支持更多语言模型
2. **Web 界面**：开发简单的 Web 界面
3. **批量处理**：实现高效的批量翻译
4. **系统优化**：全面优化系统性能和稳定性

## 🔮 未来规划

### 可能的改进方向
1. **多模型支持**：添加更多语言模型和翻译方向
2. **批量处理**：支持文件批量翻译
3. **Web 界面**：添加简单的 Web 界面
4. **性能优化**：多卡并行，提高处理速度

### 技术升级
1. **模型升级**：尝试更大的模型（如 Qwen3-14B）
2. **框架升级**：跟进 vLLM 最新版本
3. **硬件优化**：充分利用 4 卡并行能力

### 学习计划
1. **vLLM 推理与服务** - 完成基础部署
2. **指令模型加载与推理** - transformers vs vLLM 对比
3. **QLoRA 微调最小闭环** - 小样本微调实验
4. **评估与部署** - OpenAI 兼容接口、并发、流式
5. **扩展功能** - VLM 与视觉生成/微调（可选）

## 📊 项目里程碑

### 里程碑1：基础环境搭建（第1周）
- [x] CUDA 环境配置
- [x] conda 环境创建
- [x] 基础库安装
- [x] vLLM 安装验证

### 里程碑2：vLLM 服务部署（第2周）
- [ ] 模型下载和加载
- [ ] API 服务启动
- [ ] 基本功能测试
- [ ] 性能基准测试

### 里程碑3：翻译功能实现（第3周）
- [ ] 翻译脚本开发
- [ ] 质量评估机制
- [ ] 批量处理功能
- [ ] 日志和监控

### 里程碑4：系统优化（第4周）
- [ ] 性能优化
- [ ] 稳定性改进
- [ ] 文档完善
- [ ] 部署指南

## 🎯 成功标准

### 技术标准
- **服务稳定性**：7×24小时稳定运行
- **响应时间**：平均响应时间 < 1秒
- **翻译质量**：准确率 > 90%
- **并发能力**：支持 10+ 并发请求

### 学习标准
- **技术掌握**：熟练使用 vLLM 和相关工具
- **问题解决**：能够独立解决常见问题
- **系统设计**：理解系统架构和设计原则
- **文档能力**：能够编写清晰的技术文档

### 实用标准
- **功能完整**：实现完整的翻译服务
- **易于使用**：提供简单的使用接口
- **可扩展性**：便于添加新功能
- **可维护性**：代码结构清晰，便于维护

---

**项目状态**: 🚧 进行中  
**最后更新**: 2024-08-28
