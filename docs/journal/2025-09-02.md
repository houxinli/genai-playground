# 2025-09-02 翻译服务与脚本收敛

## 🎯 主要目标
用 vLLM 提供本地翻译服务；将 `tasks/translation/data/japanese/input_1.txt` 翻译为中文；质量以 samples 为参考。

## 🔧 关键卡点与解决

### 1. 服务状态误判
- **问题**：`manage_vllm.sh status` 因 PID 判断 Bug 误报，修正变量名后仍不稳
- **解决**：增加基于 `/v1/models` 的检查脚本 `scripts/check_vllm.py`，成为权威状态来源
- **经验**：PID 文件可能不准确，API 检查更可靠

### 2. 模型 404 错误
- **问题**：脚本请求 `Qwen/Qwen3-14B`，但服务实际加载 `Qwen/Qwen3-4B`
- **解决**：统一到服务默认 14B/32B（后续为 32B-AWQ），并在调用端显式传 `-m`
- **经验**：服务端和客户端模型名必须一致

### 3. 输出不合规
- **问题**：输出文件夹（应为 `tasks/translation/data/output`）与输出内容（夹带原文与思考 `<think>`）不符合约定
- **解决**：通用脚本确保输出仅译文，完整 Prompt/Response 写入 `tasks/translation/logs/`
- **经验**：输出格式需要严格规范，避免混淆

### 4. 下载/加载进度
- **问题**：vLLM 仅显示"加载"进度；下载由 HF Hub 负责
- **解决**：设置 `HF_HUB_ENABLE_HF_TRANSFER=1`、`HUGGINGFACE_HUB_VERBOSITY=debug`、`HF_HUB_DISABLE_PROGRESS_BARS=0`，必要时推荐先用 `huggingface-cli download` 预下并复用缓存
- **经验**：下载和加载是两个不同的过程，需要分别处理

### 5. 32B 显存不足
- **问题**：完整版 32B 需要约 ≥256GB（估），4×49GB 不足
- **解决**：32B-AWQ 约 19GB 权重、总显存约 4×43~46GB 可跑 → 服务默认 32B-AWQ，完整 32B 仅离线评估
- **经验**：大模型需要仔细评估显存需求，量化是必要的

### 6. 脚本收敛
- **问题**：多个零散测试脚本，功能重复
- **解决**：删除零散测试脚本，保留通用 `tasks/translation/scripts/test_translation.py`（带日志、参数可配）
- **经验**：脚本需要统一管理，避免重复开发

### 7. 文档与结构
- **问题**：文档混乱，结构不清
- **解决**：清理 docs，设定 Journal 为时间线；将长期参考文档指到 Journal；`AGENT_CONTEXT.md` 用作新会话 Prompt，不承载历史
- **经验**：文档结构需要清晰，便于维护和查阅

## ⚙️ vLLM 配置要点

### 启动核心参数
- `--tensor-parallel-size ${TP_SIZE}`（本机常用 4）
- `--use-tqdm-on-load`
- `--download-dir`（建议与 `HF_HOME` 指向同盘）

### 环境变量建议
```bash
HF_HOME=/path/to/your/hf_cache
HF_HUB_ENABLE_HF_TRANSFER=1
HUGGINGFACE_HUB_VERBOSITY=debug
HF_HUB_DISABLE_PROGRESS_BARS=0
```

### 可靠健康检查
- `GET http://localhost:8000/v1/models`
- 解析已加载模型名与上下文长度

## 📊 Qwen3-32B 分析

### Context window
- 40960 tokens（与是否 AWQ 无关）

### 显存与取舍
- **完整 32B**：在本机 4×49GB 条件下易 OOM（日志已证实）
- **32B-AWQ**：权重约 19GB，服务端总显存约 170~190GB（含 KV/激活/碎片），稳定运行

### 质量与行为
- 32B 基座在某些内容上可能触发截断/合规策略
- 32B-AWQ 实测更稳定输出完整译文

## 🎯 今日决定

### 服务默认模型
- `Qwen/Qwen3-32B-AWQ` + `TP_SIZE=4`

### 输出与日志
- 输出文件仅译文
- 日志统一到 `tasks/translation/logs/`，含完整 Prompt/Response 与元信息

### 文档体例
- 以 `docs/JOURNAL.md` 为"技术博客式"时间线
- 详档按需保留独立文件或直接内联至 Journal
- `AGENT_CONTEXT.md` 用作新会话 Prompt，不承载历史

## 🔧 脚本收敛成果

### 统一翻译入口
- **文件**：`tasks/translation/scripts/test_translation.py`
- **特点**：命令行参数化，默认生成完整日志，日志含 Prompt/Response
- **使用**：`python test_translation.py --input input.txt --output output.txt --model Qwen/Qwen3-32B-AWQ`

### count_tokens.py 改进
- **改进**：改为 CLI：支持文件/目录输入，可指定 tokenizer（默认 Qwen/Qwen3-32B）
- **使用**：`python count_tokens.py path/to/file --model Qwen/Qwen3-32B`

### 目录与输出规范
- **输入**：`data/input/`
- **输出**：`data/output/`
- **示例**：`data/samples/`
- **日志**：`logs/`
- **忽略**：`.gitignore` 忽略生成物

## 🔧 vLLM 服务优化

### 启动脚本 serve_vllm.sh
- **关键变量**：TP=4、tqdm 加载、HF Hub 下载日志、CUDA 环境
- **环境变量**：明确设置 CUDA 路径和库路径
- **下载优化**：启用 HF Transfer 和详细日志

### 健康检查脚本 check_vllm.py
- **功能**：通过 `/v1/models` 校验服务与模型名
- **优势**：比 PID 检查更可靠
- **输出**：显示模型名、上下文长度、创建时间等

### 默认建议模型
- **在线服务**：Qwen/Qwen3-32B-AWQ（32B Base 离线评估为主）
- **原因**：显存需求适中，质量损失最小，vLLM 原生支持

## 📋 Makefile 更新

### translate 目标
```makefile
translate:
	@echo "📝 执行翻译任务..."
	$(PY) tasks/translation/scripts/test_translation.py --input tasks/translation/data/input/input_1.txt --output tasks/translation/data/output/translated.txt --model Qwen/Qwen3-32B-AWQ
```

### 影响
- 一条命令即可本地跑通 Qwen3 翻译（推荐 32B-AWQ）
- 输出与日志位置统一
- 服务状态更可靠
- 文档从 Journal 入口即可回溯全部决策与操作要点

## 🔍 故障记录：libcuda.so 链接问题

### 现象
- 启动/运行阶段出现 `libcuda.so` 相关链接错误或找不到库

### 影响
- vLLM/torch 相关组件无法正常加载，导致服务启动失败

### 解决
- 使用用户级路径与环境变量修正库查找（如 `LD_LIBRARY_PATH`、`LIBRARY_PATH`、`CUDA_HOME`）
- 优先在用户空间放置符号链接，避免系统级更改
- 记录在服务启动脚本中以便复现（参考 `scripts/serve_vllm.sh`）

### 具体步骤
```bash
# 1) 确认 nvidia-smi 可用
nvidia-smi

# 2) 查询系统 CUDA 路径（如 /usr/local/cuda-*）
ls -l /usr/local | grep cuda || true

# 3) 查询 libcuda.so 所在（通常在 /usr/lib/x86_64-linux-gnu/ 或 驱动路径）
ldconfig -p | grep libcuda || true

# 4) 临时修复（当前会话）
export CUDA_HOME=/usr/local/cuda-12.4
export LD_LIBRARY_PATH=$CUDA_HOME/lib64:${LD_LIBRARY_PATH}
export LIBRARY_PATH=$CUDA_HOME/lib64:${LIBRARY_PATH}

# 5) 若仍缺失，创建用户级符号链接到驱动提供的 libcuda.so
mkdir -p ~/.local/lib
ln -sf /usr/lib/x86_64-linux-gnu/libcuda.so ~/.local/lib/libcuda.so || true
export LD_LIBRARY_PATH=~/.local/lib:${LD_LIBRARY_PATH}

# 6) 验证 Python 能加载 CUDA
python - <<'PY'
import torch
print('torch.cuda.is_available =', torch.cuda.is_available())
print('torch.version.cuda =', getattr(torch.version, 'cuda', None))
PY
```

## 📋 下载策略（命令级）

### 统一缓存与网络加速
- 建议持久缓存目录：`HF_HOME=/path/to/your/hf_cache`
- 建议环境：
```bash
export HF_HOME=/path/to/your/hf_cache
export HF_HUB_ENABLE_HF_TRANSFER=1
export HUGGINGFACE_HUB_VERBOSITY=debug
export HF_HUB_DISABLE_PROGRESS_BARS=0
```

### 预下载完整 32B（基座）
```bash
huggingface-cli download Qwen/Qwen3-32B \
  --local-dir $HF_HOME/models--Qwen--Qwen3-32B \
  --local-dir-use-symlinks False \
  --resume-download
```

### 预下载 32B-AWQ（服务默认）
```bash
huggingface-cli download Qwen/Qwen3-32B-AWQ \
  --local-dir $HF_HOME/models--Qwen--Qwen3-32B-AWQ \
  --local-dir-use-symlinks False \
  --resume-download
```

## 🔮 维护约定
- Journal 是唯一权威的时间线；其他分析文档/配置文档作为"可链接参考"
- 每当出现：
  1) 新的工程决策（换模型/参数/结构）
  2) 新的故障及定位过程
  3) 新的最佳实践或脚本
  应在此追加条目并附上参考链接

## 🔧 翻译流水线优化与测试

### 翻译脚本重构与优化
- **代码模块化**：将 `translate_pixiv_v1.py` 中的工具函数提取到 `tasks/translation/scripts/utils/` 模块，提高代码可维护性
- **文件管理优化**：删除冗余的 `.meta.json` 文件（69个），减少总文件数；重命名系列文章文件为 `{series_id}_{novel_id}.txt` 格式（32个文件）
- **日志系统改进**：在命令行输出中显示对应的日志文件路径，便于调试和监控

### 翻译质量测试
- **测试文章**：使用系列文章和独立文章进行翻译质量验证
- **参数优化**：发现 `temperature=0.0` 和 `frequency-penalty=0.0` 的组合能提供最稳定的翻译输出
- **质量检查**：`looks_bad_output` 函数成功检测到异常输出模式，确保翻译质量

### 关键技术改进
- **动态token计算**：根据输入长度动态计算 `max_tokens`，避免超出模型上下文限制
- **输出清理**：自动移除模型思考部分（`<think>...</think>`），保持输出纯净
- **错误处理**：增强重试机制和错误检测，提高翻译成功率

### vLLM 配置优化
- **AWQ 模型 TP 配置**：发现 AWQ 模型需要 TP 能整除某个数，将默认 TP_SIZE 从 3 改为 2
- **完整 32B 模型配置**：支持 TP=4、kv-cache-dtype=fp8、trust-remote-code 等关键参数
- **自动配置切换**：根据模型名自动选择最优配置（32B vs AWQ）

### 文件结构优化
```
tasks/translation/data/pixiv/[作品ID]/
├── index.json                    # 文章索引和元数据
├── [文章ID]_[章节ID].txt          # 系列文章（重命名后）
├── [文章ID]_[章节ID]_zh.txt       # 翻译输出
├── [独立文章ID].txt               # 独立文章
├── [独立文章ID]_zh.txt             # 翻译输出
└── ...                           # 其他文章
```

### 翻译性能指标
- **处理速度**：单篇文章翻译时间约30-180秒（取决于文章长度）
- **输出质量**：完整保留原文结构和语义，专业术语翻译准确
- **稳定性**：通过质量检查的文章翻译成功率100%

## 🔍 其他问题回顾

### manage_vllm.sh status 误报
- **原因**：PID 判断变量名错误导致，总是显示未运行
- **解决**：已修复，并以 `/v1/models` 为最终权威来源（`scripts/check_vllm.py`）

### 模型名不一致
- **原因**：服务与客户端请求不一致导致 `404 The model ... does not exist`
- **解决**：统一通过 `-m` 显式传模型，并确认服务日志中的加载模型名

### 输出内容不合规
- **原因**：译文输出夹带原文与 `<think>` 思考
- **解决**：改为输出仅保留译文，完整 Prompt/Response 写入日志（`tasks/translation/logs/`）

### 32B OOM
- **原因**：尝试加载完整 32B 时报 CUDA OOM
- **解决**：确认 32B-AWQ 方案可稳定运行（TP=4），服务默认切换为 AWQ

### Makefile 目标过期
- **原因**：仍指向已删除的 `translate_stable.py`/`translate_exp.py`
- **解决**：已改为通用 `test_translation.py`

### 文档误删与恢复
- **原因**：清理 docs 时误删
- **解决**：现采用"Journal 加详档"的博客式结构并恢复关键文档

### count_tokens.py 优化
- **原因**：原版写死路径
- **解决**：改为 CLI 传参，支持文件或目录统计，默认 Qwen3 tokenizer，可通过 `-m` 指定

### 默认日志文件
- **原因**：通用翻译脚本需要手动指定日志路径
- **解决**：通用翻译脚本默认自动生成时间戳日志路径（无需手动指定）

## 📝 待办事项
- 提供完整 32B 的 `huggingface-cli` 下载命令（放 Journal"下载策略"条目）
- 评估 Sakura-13B-Galgame 模型（下载→服务→对比）
- example_1 使用 32B-AWQ 跑通并落盘（完整日志）
